1
00:00:04,630 --> 00:00:05,710
What's going on, everybody?

2
00:00:05,950 --> 00:00:09,370
Welcome to part six of the Neural Networks
from Scratch video series.

3
00:00:09,890 --> 00:00:13,070
In this video, what we're going to be
talking about in covering is the Softmax

4
00:00:13,071 --> 00:00:18,010
activation function, which is specifically
used for the output layer on our

5
00:00:18,160 --> 00:00:20,790
classification style neural network
models.

6
00:00:21,595 --> 00:00:24,279
Before we get into that, a
quick update to the Neural

7
00:00:24,280 --> 00:00:27,110
Networks from Scratch
book is now fully released.

8
00:00:27,470 --> 00:00:30,090
We have a hardcover, which you see here,
as well as a softcover.

9
00:00:30,525 --> 00:00:32,490
Also, the e-book now has a PDF download.

10
00:00:33,130 --> 00:00:37,890
All books give you access to that PDF and
e-book, and we still also have the Google

11
00:00:37,891 --> 00:00:40,178
Docs, so you can
still highlight and ask

12
00:00:40,179 --> 00:00:42,530
questions in line with
the text and all that.

13
00:00:43,050 --> 00:00:48,330
If you're interested in any version of the
book, you can get it at nnfs.io.

14
00:00:49,005 --> 00:00:52,710
I am going to be coding everything from
the book, so you should be able to watch

15
00:00:52,711 --> 00:00:57,270
the videos, read the book, and kind of cross
back and forth with very little effort.

16
00:00:57,745 --> 00:01:00,350
That way, if maybe you've coded something
from the video, and then you're still

17
00:01:00,351 --> 00:01:02,166
maybe a little confused, you want to get a
refresher.

18
00:01:02,190 --> 00:01:02,470
You could go to the end of the video.

19
00:01:02,471 --> 00:01:06,050
You could go in theory back to the book,
for example, and kind of go through the

20
00:01:06,051 --> 00:01:09,310
exact same code, just in written form,
or vice versa, and so on.

21
00:01:09,680 --> 00:01:14,930
So anyway, we're going to be starting off
here with the softmax activation function.

22
00:01:16,655 --> 00:01:20,810
And the first question
that I'm sure many people

23
00:01:20,811 --> 00:01:23,450
will have is, why another
activation function?

24
00:01:23,610 --> 00:01:25,196
Why don't we just
continue to use something

25
00:01:25,197 --> 00:01:27,231
like the rectified linear
activation function?

26
00:01:27,270 --> 00:01:32,140
So I think to start us off, I would say,
let's just... I'm going to start with this.

27
00:01:32,141 --> 00:01:32,050
..

28
00:01:32,051 --> 00:01:32,450
a fresh, fresh, fresh, fresh, fresh,
fresh, fresh, fresh, fresh, fresh,

29
00:01:32,451 --> 00:01:32,450
fresh, fresh, fresh, fresh, fresh,
fresh, fresh, fresh, fresh, fresh,

30
00:01:32,451 --> 00:01:32,450
fresh, fresh, fresh, fresh, fresh,
fresh.

31
00:01:33,405 --> 00:01:37,890
Just for illustration purposes,
and then once we actually code the code

32
00:01:37,891 --> 00:01:42,210
that we intend to continue with our...
essentially, we're building a framework here.

33
00:01:42,410 --> 00:01:45,210
So for that code, we'll go back to the
code that we've been building.

34
00:01:45,870 --> 00:01:47,630
But for now, we're just going to start
fresh.

35
00:01:48,270 --> 00:01:53,050
So let's just say you've got some layer
outputs, and the values can be anything.

36
00:01:53,310 --> 00:01:55,350
We're just going to make them up, but I
am going to use the values from the book.

37
00:01:56,025 --> 00:01:57,930
4.8, 1.21, 2.385.

38
00:01:59,050 --> 00:02:01,450
So imagine these are your output values.

39
00:02:02,820 --> 00:02:03,820
What do we do with that?

40
00:02:03,930 --> 00:02:06,370
So if we're only predicting, it's pretty
simple.

41
00:02:06,930 --> 00:02:09,335
You do... you just... you
would say the prediction is

42
00:02:09,336 --> 00:02:12,070
whichever one of these
values is the... is the largest.

43
00:02:12,880 --> 00:02:14,361
So in this case, it would be index 0.

44
00:02:14,890 --> 00:02:17,670
So let's assume index 0 is indeed the
intended target.

45
00:02:18,550 --> 00:02:19,550
Awesome.

46
00:02:19,610 --> 00:02:20,786
You've got your prediction right.

47
00:02:20,810 --> 00:02:25,310
But what we're trying to learn how to do
is actually train neural networks as well.

48
00:02:25,510 --> 00:02:28,890
So with a... you don't have to think about
this if all you're doing is predicting,

49
00:02:29,030 --> 00:02:31,710
but someone had to think about it at some
point to train this model.

50
00:02:31,990 --> 00:02:36,310
So if you want to train a model,
the... you know, there's a whole bunch

51
00:02:36,311 --> 00:02:41,030
that goes on that we will get to and that
we'll take up many videos and many hours

52
00:02:41,031 --> 00:02:43,210
of your time to figure out and learn how
this works.

53
00:02:45,440 --> 00:02:50,232
But at this point, the
first step in training a

54
00:02:50,233 --> 00:02:54,870
model is to determine how
wrong is this model, right?

55
00:02:55,170 --> 00:02:57,510
Because accuracy is not a good indication
of that.

56
00:02:58,290 --> 00:03:06,290
So for example, if we change some of these
values, maybe 4.79 and 4.25.

57
00:03:06,550 --> 00:03:07,550
I don't know.

58
00:03:07,860 --> 00:03:10,970
Which one of these is more correct?

59
00:03:11,910 --> 00:03:14,670
Well, I think probably most people would
agree.

60
00:03:15,190 --> 00:03:21,990
This first one is more correct because
relatively this neuron zero here is

61
00:03:21,991 --> 00:03:26,890
relatively larger than the other neurons
in this list.

62
00:03:27,090 --> 00:03:28,811
Whereas here, they're actually a lot
closer.

63
00:03:28,970 --> 00:03:30,690
Accuracy wise, they're identical.

64
00:03:32,250 --> 00:03:36,366
So in order to measure how
wrong something is, the first order

65
00:03:36,367 --> 00:03:39,250
of business is to compare
them relatively to other neurons.

66
00:03:39,630 --> 00:03:42,071
But the problem with
this is like the rectified

67
00:03:42,072 --> 00:03:45,070
linear activation function,
for example, is exclusive.

68
00:03:45,650 --> 00:03:46,650
It's per neuron.

69
00:03:46,910 --> 00:03:48,510
They're not really connected in any way.

70
00:03:48,570 --> 00:03:51,950
So there is no relative comparison that
you can really fairly make.

71
00:03:52,650 --> 00:03:56,050
The next problem is these are unbounded.

72
00:03:56,350 --> 00:04:01,679
So the relative closeness
can vary considerably between

73
00:04:01,729 --> 00:04:04,600
each sample that you pass
through this neural network.

74
00:04:05,030 --> 00:04:06,630
So we've got so many issues.

75
00:04:06,870 --> 00:04:08,310
There's like a bounding issue.

76
00:04:08,790 --> 00:04:09,831
Every neuron is exclusive.

77
00:04:10,575 --> 00:04:12,950
They don't really relate to each other.

78
00:04:13,350 --> 00:04:20,150
And we don't really have a good solid way
of determining how wrong is this in any

79
00:04:20,151 --> 00:04:24,690
formal uniform way per sample that comes
through.

80
00:04:24,930 --> 00:04:30,510
So this is the problem, and this is why we
need some new activation function,

81
00:04:30,511 --> 00:04:32,930
which in this case we're
going to be using the

82
00:04:32,931 --> 00:04:35,090
softmax activation
function to help us solve.

83
00:04:35,545 --> 00:04:36,670
Now, why softmax?

84
00:04:37,270 --> 00:04:41,530
Well, we need to kind of, again, kind of
consider what's our end objective here.

85
00:04:41,830 --> 00:04:47,930
So zooming out to a full model for the
moment, what actually do we want to happen?

86
00:04:48,110 --> 00:04:49,666
So let's say we've got some image data.

87
00:04:49,690 --> 00:04:53,670
We're going to pass that through the neural
network, and then we get the output values.

88
00:04:53,950 --> 00:04:55,610
Now, what do we want those values to be?

89
00:04:55,850 --> 00:04:59,810
So ideally, these values would be a
probability distribution.

90
00:05:00,830 --> 00:05:03,770
This gives us a few things.

91
00:05:03,910 --> 00:05:07,330
First of all, this means everything will
be uniform from sample to sample.

92
00:05:07,331 --> 00:05:10,390
Also, from neuron to neuron, things will
be normalized.

93
00:05:11,450 --> 00:05:15,310
And we can actually calculate rightness
and wrongness, so to speak.

94
00:05:15,590 --> 00:05:20,310
So in this case, the correct
classification, if everything was perfect,

95
00:05:21,070 --> 00:05:25,930
the correct classification would be a 1.0,
and all the other neurons would read out

96
00:05:26,080 --> 00:05:27,080
0.0.

97
00:05:27,850 --> 00:05:32,930
So now we can actually begin to measure
how right or how wrong are we.

98
00:05:32,931 --> 00:05:35,152
So we know this is where
we want to get, and the

99
00:05:35,153 --> 00:05:38,050
question is, how do we
actually get to that point?

100
00:05:38,620 --> 00:05:42,770
So one immediate idea here might be,
well, all you need to really do is

101
00:05:42,771 --> 00:05:47,630
normalize the values and create that
distribution by taking each neuron's value

102
00:05:47,631 --> 00:05:51,910
and divide that value by the total value
of all the neurons in that layer.

103
00:05:52,560 --> 00:05:53,681
And then you're done, right?

104
00:05:53,970 --> 00:05:57,150
So let's say you just use a probability
distribution, and you keep old,

105
00:05:57,350 --> 00:05:59,510
faithful, rectified linear activation
function.

106
00:05:59,511 --> 00:06:04,950
The problem here is if any of those values
in the output layer is a negative,

107
00:06:05,570 --> 00:06:07,407
the rectified linear
activation function is

108
00:06:07,408 --> 00:06:10,570
going to clip it, and
it's just going to be a 0.

109
00:06:10,870 --> 00:06:13,510
And then when we go to create a
probability distribution from that,

110
00:06:13,870 --> 00:06:19,650
it will always be 0, whether it was a
negative 20 or negative 9,000.

111
00:06:20,255 --> 00:06:23,590
And then the other problem is, what if
all of the values are in the negatives?

112
00:06:23,765 --> 00:06:28,810
There would be impossible to learn from
here, and again, we haven't quite yet

113
00:06:28,811 --> 00:06:35,110
reached back propagation, but learning
from clipped values, quote unquote,

114
00:06:35,290 --> 00:06:39,690
how wrong or how right is something,
is very difficult because there is no

115
00:06:39,691 --> 00:06:41,771
meaning once you've clipped, you've lost
all meaning.

116
00:06:42,010 --> 00:06:44,970
Was it a negative 20 before it got
clipped, or was it a negative 1 million?

117
00:06:45,410 --> 00:06:46,410
We don't know.

118
00:06:46,590 --> 00:06:50,170
So then you might suggest, let's just use
a linear activation function.

119
00:06:50,410 --> 00:06:51,490
Let's basically do nothing.

120
00:06:52,135 --> 00:06:54,743
The problem is, you're
still going to need to figure

121
00:06:54,744 --> 00:06:57,311
out what the heck do you
do with these negative values.

122
00:06:57,855 --> 00:06:59,040
So you might throw
your hands up at this point

123
00:06:59,041 --> 00:07:01,511
and be like, let's just
use absolute values.

124
00:07:01,630 --> 00:07:03,716
Or let's square the
output, then we'll just

125
00:07:03,717 --> 00:07:06,591
solve this whole negative
problem altogether.

126
00:07:06,855 --> 00:07:09,651
The problem with this
is, after you have these

127
00:07:09,652 --> 00:07:11,970
values, you need to be
able to back propagate.

128
00:07:12,210 --> 00:07:16,830
You need to be able to have an optimizer
intelligently optimize your variables.

129
00:07:16,831 --> 00:07:19,332
Well, if you have a
negative 9 that got absolute

130
00:07:19,333 --> 00:07:22,370
value to a 9, that's a big
difference than a 9, right?

131
00:07:22,930 --> 00:07:25,890
So the variables that you would need to
tweak and the directions that you would

132
00:07:25,891 --> 00:07:29,770
need to tweak these variables to get those
changes that you want would vary.

133
00:07:30,250 --> 00:07:33,370
So, we can't just lose the meaning.

134
00:07:33,610 --> 00:07:37,210
A negative 9 is not the same as a positive
9.

135
00:07:37,920 --> 00:07:39,650
So, what the heck do we do?

136
00:07:41,165 --> 00:07:42,270
Enter exponentiation.

137
00:07:42,590 --> 00:07:47,910
So the exponential function is y or your
output is equal to e, or Euler's number,

138
00:07:48,370 --> 00:07:50,770
raised to the power of x, which is your
input.

139
00:07:51,535 --> 00:07:55,750
Euler's number is approximately 2.71828
and so on.

140
00:07:56,580 --> 00:08:01,850
And exponentiation is the act of applying
the exponential function to some value.

141
00:08:01,851 --> 00:08:06,190
So, what this does for us is it solves our
negatives issue by making sure no

142
00:08:06,191 --> 00:08:09,054
negative, or really no
value, can be negative

143
00:08:09,055 --> 00:08:12,331
at the output of the
exponential function.

144
00:08:12,545 --> 00:08:16,650
And it does this while not tossing away the
value or the meaning of that negativity.

145
00:08:16,970 --> 00:08:18,790
It's still on a scale, let's say.

146
00:08:18,950 --> 00:08:24,050
So, the exponentiation of 1.1 is 3 or 3 in
some change.

147
00:08:24,540 --> 00:08:29,790
The exponentiation of negative 1.1 is
0.3329 or so.

148
00:08:30,640 --> 00:08:35,490
So, this is more than just using absolute
values or squaring the value, for example.

149
00:08:36,110 --> 00:08:39,250
Now, also, in theory, you don't have to
use Euler's number here, and you would

150
00:08:39,251 --> 00:08:41,410
still solve all your problems up to this
point.

151
00:08:42,110 --> 00:08:44,905
But Euler's number is actually
going to be coming handy later

152
00:08:44,906 --> 00:08:47,530
on, and we'll be sure to
reference that when we get there.

153
00:08:47,670 --> 00:08:51,390
But this is how we're solving this problem
of negativity.

154
00:08:52,270 --> 00:08:54,124
Alright, getting back
to our code, let's code

155
00:08:54,125 --> 00:08:57,410
the raw Python implementation
for exponentiation.

156
00:08:57,411 --> 00:09:00,190
So, to do this, we're just going to keep
the original layer outputs.

157
00:09:01,230 --> 00:09:01,904
And first, we need
a definition for E, the

158
00:09:01,905 --> 00:09:11,970
constant Euler's number,
and that is 2.7182846.

159
00:09:12,650 --> 00:09:13,650
I hope I got that right.

160
00:09:14,090 --> 00:09:19,170
Also, if you are in Python, you can always
import math, and then rather than hard

161
00:09:19,171 --> 00:09:23,570
coding Euler's number that way,
you could say E equals math.E.

162
00:09:24,630 --> 00:09:26,170
And that would be the same thing.

163
00:09:26,390 --> 00:09:29,226
But if they are following along in some
other language, and you don't have that

164
00:09:29,250 --> 00:09:30,965
kind of math library
that just gives you the

165
00:09:30,966 --> 00:09:33,891
number, you could just
hard code it in that way.

166
00:09:34,050 --> 00:09:39,210
So, once we have that number, we want to
exponentiate all each of these values.

167
00:09:39,610 --> 00:09:41,670
So, doing that is really basic Python
here.

168
00:09:41,790 --> 00:09:44,710
So, we're going to say X values,
we're going to make that an empty list.

169
00:09:45,460 --> 00:09:47,660
And then we're just going to iterate over
layer outputs.

170
00:09:47,990 --> 00:09:56,544
So, for output in layer outputs, we're
going to X values dot append E to the power.

171
00:09:56,545 --> 00:09:56,390
..

172
00:09:56,391 --> 00:10:00,830
Actually, it's capital E to the power of
whatever that output is.

173
00:10:01,110 --> 00:10:04,750
And then when we're all done, we can print
X values.

174
00:10:05,430 --> 00:10:06,210
Let's go ahead and run that.

175
00:10:06,410 --> 00:10:09,660
And we have our
exponentiated values at this point,

176
00:10:09,661 --> 00:10:12,571
checking the book to
make sure it is identical.

177
00:10:12,610 --> 00:10:13,610
And it looks like it is.

178
00:10:13,760 --> 00:10:14,760
Okay, great.

179
00:10:14,910 --> 00:10:15,990
So, that's exponentiation.

180
00:10:16,310 --> 00:10:18,010
Really, not much to it at that point.

181
00:10:18,630 --> 00:10:23,970
So, the next step, once we've exponentiated
these values, is to normalize the values.

182
00:10:24,250 --> 00:10:26,210
So, what do we mean by normalize the
values?

183
00:10:26,580 --> 00:10:31,650
So, in our case, it's going to be a single
output neurons value, divided by the sum

184
00:10:31,651 --> 00:10:35,130
of all of the other output neurons in that
output layer.

185
00:10:35,925 --> 00:10:39,830
And this gives us the probability
distribution that we want.

186
00:10:40,480 --> 00:10:43,628
But we still want to exponentiate
before this point, because

187
00:10:43,629 --> 00:10:46,710
again, we need to get rid of
all of these negative values.

188
00:10:47,425 --> 00:10:51,230
But we do not want to lose the meaning of
the negative value.

189
00:10:51,510 --> 00:10:55,568
So, we're exponentiating to
convert negatives to positives

190
00:10:55,569 --> 00:10:58,350
without actually losing the
meaning of a negative value.

191
00:11:00,420 --> 00:11:03,640
So, continuing along in our
raw Python implementation

192
00:11:03,641 --> 00:11:05,930
here, let's go ahead
and code in normalization.

193
00:11:05,931 --> 00:11:09,431
So, again, normalization occurs
after we've done exponentiation,

194
00:11:09,432 --> 00:11:12,210
which will rid us of these
pesky negative values.

195
00:11:13,240 --> 00:11:15,380
So, to do the normalization,
again, it's a single

196
00:11:15,381 --> 00:11:17,330
value divided by the
total of all the values.

197
00:11:17,470 --> 00:11:18,870
So, we've got exponential values.

198
00:11:19,110 --> 00:11:24,611
So, what we're going to say is norm base
equals the sum of those exponential values.

199
00:11:25,230 --> 00:11:27,710
And then we're going to say norm values.

200
00:11:28,475 --> 00:11:35,490
That was, yeah.

201
00:11:35,491 --> 00:11:39,870
We're going to do norm values dot append.

202
00:11:40,570 --> 00:11:46,610
And it's just going to be that value
divided by the norm base.

203
00:11:47,785 --> 00:11:50,870
And this will give us the normalized
value.

204
00:11:50,871 --> 00:11:54,410
So, we can print norm values.

205
00:11:54,835 --> 00:11:57,295
And then also, this
should add up, it should

206
00:11:57,296 --> 00:11:59,290
add up to one, or
very, very close to one.

207
00:11:59,490 --> 00:12:05,570
So, print a sum of norm values.

208
00:12:05,950 --> 00:12:06,950
We'll run that.

209
00:12:07,370 --> 00:12:10,150
And what we get is here is our probability
distribution.

210
00:12:10,370 --> 00:12:14,910
So, this first, this first kind of list
here, this is our exponentiated values.

211
00:12:15,270 --> 00:12:17,510
Then this is our normalized exponentiated
values.

212
00:12:17,900 --> 00:12:21,690
And then this just sums it up and shows
that it does indeed add up to basically one.

213
00:12:22,165 --> 00:12:25,090
Those values should be pretty close to
what's in the book.

214
00:12:25,340 --> 00:12:28,850
It actually isn't exactly, but this isn't
using NumPy.

215
00:12:29,740 --> 00:12:32,220
So, the values should line up,
I think, when we're using NumPy.

216
00:12:33,390 --> 00:12:35,190
We'll confirm that when we get there.

217
00:12:36,540 --> 00:12:39,690
So, the n and fs, first of all, we're not
even initializing the n and fs package.

218
00:12:40,270 --> 00:12:42,290
But that is part of what the init package
is doing.

219
00:12:42,510 --> 00:12:42,591
But here, interestingly
enough, even raw

220
00:12:42,592 --> 00:12:51,670
Python seems to vary
from machine to machine.

221
00:12:51,671 --> 00:12:54,270
I don't think it actually matters at this
stage.

222
00:12:56,270 --> 00:12:58,299
So, they're close, but
it's like as you get to some

223
00:12:58,359 --> 00:13:00,490
of these ladder decimals,
that's where it's differing.

224
00:13:00,950 --> 00:13:04,110
So, that's the raw Python implementation.

225
00:13:04,610 --> 00:13:06,603
And again, if you're coming
from some other language,

226
00:13:06,604 --> 00:13:08,850
you would need to implement
this in your language.

227
00:13:09,510 --> 00:13:12,550
But now the next thing that we're going to
do is convert this to NumPy.

228
00:13:12,750 --> 00:13:17,443
So, what we're going
to do is take, I guess,

229
00:13:17,444 --> 00:13:21,291
in our code, we
probably are retaining e.

230
00:13:21,510 --> 00:13:24,130
But we can keep math.e there, outputs
there.

231
00:13:24,350 --> 00:13:29,210
We're going to go ahead and import NumPy
as np.

232
00:13:30,220 --> 00:13:39,445
And rather than doing, really, all of this,
we delete, and x values just becomes np.

233
00:13:39,446 --> 00:13:40,190
x.

234
00:13:41,185 --> 00:13:44,990
And what are we applying this exponential
function to?

235
00:13:45,630 --> 00:13:46,630
Layer outputs.

236
00:13:46,690 --> 00:13:51,030
So, by default, typically, NumPy
functions, what they're going to do is,

237
00:13:51,031 --> 00:13:55,530
first, they will just by default,
impact every value.

238
00:13:56,550 --> 00:14:01,390
And if you want it to be in a little bit
more specific way, you can become more

239
00:14:01,391 --> 00:14:03,672
specific, and we'll actually be showing
that very shortly.

240
00:14:04,860 --> 00:14:09,730
But by default, if you just do this, it's
going to apply this to each value in total.

241
00:14:10,470 --> 00:14:12,630
So, that's a quicker way to get our
exponential values.

242
00:14:13,030 --> 00:14:18,090
And then for the normalization values,
all we need to do at this point is,

243
00:14:19,110 --> 00:14:24,044
we're just going to say, norm
underscore values is equal

244
00:14:24,045 --> 00:14:31,830
to the x values divided by
the np.sum of the x values.

245
00:14:32,350 --> 00:14:35,290
So, let's go ahead and run that real
quick, and then we see that we do indeed

246
00:14:35,291 --> 00:14:38,670
still get the same value or same ish
values.

247
00:14:39,195 --> 00:14:42,150
And this time, it actually does line up
with what's in the book, but if you wanted

248
00:14:42,151 --> 00:14:49,130
to be super particular, I would imagine,
we would import nnfs and nnfs.in it.

249
00:14:50,210 --> 00:14:52,082
These values actually
do line up with what is in

250
00:14:52,083 --> 00:14:53,750
the book, but let's just
run that one more time.

251
00:14:54,130 --> 00:14:57,090
And so, if you did have any variance
there, hopefully that would change it.

252
00:14:57,890 --> 00:15:00,090
So, okay, so that lines up exactly.

253
00:15:00,570 --> 00:15:02,406
And as you can see, it's a little shorter
code.

254
00:15:02,430 --> 00:15:04,190
I think it's a little more legible.

255
00:15:05,675 --> 00:15:13,070
But, yeah, so that is our exponentiation
and then our normalization.

256
00:15:13,370 --> 00:15:17,430
So, to sum up everything up to this point
for our coding of the softmax activation

257
00:15:17,431 --> 00:15:21,610
function, we have input, right,
which is actually going to be the output

258
00:15:21,611 --> 00:15:25,390
layer of data that we're going to input
into this activation function.

259
00:15:26,255 --> 00:15:31,330
We exponentiate those input values,
so each one uniquely gets exponentiated,

260
00:15:31,870 --> 00:15:35,550
then we normalize, and then that becomes
our output.

261
00:15:38,950 --> 00:15:44,650
So, the combination of this exponentiation
and normalization is what makes up the

262
00:15:44,651 --> 00:15:48,850
softmax activation function, the formula
for which is this.

263
00:15:49,730 --> 00:15:53,570
And hopefully this formula is actually
pretty simple to understand at this point,

264
00:15:53,670 --> 00:15:55,864
since you've already
seen the code that goes into

265
00:15:55,865 --> 00:15:58,270
it, and it's not really
complicated in any way.

266
00:15:59,410 --> 00:16:03,230
Okay, so at this point, we know everything
that goes into the softmax activation

267
00:16:03,231 --> 00:16:09,950
function, and now it is just a function of
actually applying and implementing this in

268
00:16:09,951 --> 00:16:13,390
such a way that makes sense for our actual
neural network applications.

269
00:16:14,340 --> 00:16:19,090
So, the main issue that we have at this
stage is we are currently working with a

270
00:16:19,091 --> 00:16:22,510
single kind of vector here of a layer's
outputs.

271
00:16:22,950 --> 00:16:25,950
When really, we're not going to have a
single layer of outputs, we're actually

272
00:16:25,951 --> 00:16:30,250
not going to have a single output from a
layer.

273
00:16:30,630 --> 00:16:33,230
We're going to have a batch of these
outputs, because we're going to have a

274
00:16:33,231 --> 00:16:36,970
batch of inputs, and that's going to
produce a batch of outputs.

275
00:16:37,070 --> 00:16:43,010
So, the next thing we want to do is
convert this, and really all this,

276
00:16:43,470 --> 00:16:45,390
to work as a batch.

277
00:16:45,860 --> 00:16:50,170
So to do that, first let's make this an
actual batch.

278
00:16:51,330 --> 00:16:55,930
And again, I am going to use the same
values that we have in the book.

279
00:16:56,510 --> 00:17:01,070
So, let me paste and paste.

280
00:17:05,210 --> 00:17:09,139
And the second output
here is an 8.9 negative 1.81

281
00:17:09,239 --> 00:17:18,670
0.2, and then down here
we get a 1.41 1.0510.026.

282
00:17:19,730 --> 00:17:20,730
Okay.

283
00:17:21,170 --> 00:17:23,030
Also, we can kind of clean up here.

284
00:17:23,210 --> 00:17:26,015
I just realized if we still
have math and math.e, we

285
00:17:26,035 --> 00:17:30,250
don't need that anymore,
because this handles it for us.

286
00:17:31,000 --> 00:17:32,970
So, how do we convert to batch?

287
00:17:33,570 --> 00:17:38,170
So for exponentiating, it turns out we
don't actually need to do anything,

288
00:17:38,620 --> 00:17:46,050
because these NumPy functions here work by
default at the individual value level.

289
00:17:46,200 --> 00:17:50,389
So, if we print x
values, I thought I did, I

290
00:17:50,409 --> 00:17:54,210
thought I already completed
that, I guess I didn't.

291
00:17:54,470 --> 00:17:55,750
There we have our values.

292
00:17:55,990 --> 00:17:58,490
It's actually already done for us,
and it's correct.

293
00:17:59,300 --> 00:18:04,330
So, the next question is, okay,
how do we do this step?

294
00:18:06,960 --> 00:18:11,390
So, x values, we don't need to change
anything there for a batch, but for some,

295
00:18:11,930 --> 00:18:12,930
we do.

296
00:18:13,260 --> 00:18:18,010
So, to illustrate sum, rather than doing
the sum of the exponential values,

297
00:18:18,050 --> 00:18:19,750
it's kind of hard to visually add these
up.

298
00:18:20,880 --> 00:18:27,110
We're going to do the sum of... let's do
this.

299
00:18:28,490 --> 00:18:29,930
I'm just going to comment these out.

300
00:18:30,550 --> 00:18:37,210
We are going to... let's just print np.sum
layer outputs.

301
00:18:37,910 --> 00:18:39,590
Now, remember, what do we want to do here?

302
00:18:39,930 --> 00:18:44,430
We actually want to iterate over here,
over this 2D matrix.

303
00:18:45,435 --> 00:18:51,210
We want to iterate over this, and do this
sum, this sum, this sum, and this sum.

304
00:18:51,690 --> 00:18:55,970
But by default, I already told you,
it's going to do as individual values.

305
00:18:56,495 --> 00:19:00,830
So by default, what we get is a single
scalar value.

306
00:19:01,740 --> 00:19:02,780
That's not what we wanted.

307
00:19:02,915 --> 00:19:04,210
We really want three values.

308
00:19:04,830 --> 00:19:06,070
For sure, we want three values.

309
00:19:06,590 --> 00:19:08,490
So, how do we get those three values?

310
00:19:09,350 --> 00:19:15,650
So, the first order of business is to pass
the axes parameter here.

311
00:19:15,960 --> 00:19:17,080
So, we're going to say axes.

312
00:19:18,070 --> 00:19:20,390
And the axes, by default, is actually
none.

313
00:19:21,170 --> 00:19:23,210
And that gives us the same value that we
saw before.

314
00:19:23,870 --> 00:19:25,310
Ax is zero.

315
00:19:25,810 --> 00:19:31,130
To put it extremely simply on a 2D matrix,
it's going to be the sum of columns.

316
00:19:31,730 --> 00:19:33,090
So, we're going to run this again.

317
00:19:33,310 --> 00:19:39,774
And as you can see, 15.11 is this column,
and then 0.451 is this column, and 2.

318
00:19:40,054 --> 00:19:42,570
611 is this last column.

319
00:19:45,410 --> 00:19:47,130
Unfortunately, that's not what we want.

320
00:19:47,370 --> 00:19:50,330
We actually want the sum of the rows put
simply.

321
00:19:50,331 --> 00:19:52,670
So, we can change that to axes one.

322
00:19:53,240 --> 00:19:58,190
And sure enough, what we get is what we
hoped for, which is the sum here,

323
00:19:58,960 --> 00:20:00,030
here, and here.

324
00:20:01,980 --> 00:20:03,420
Now, we still have a slight problem.

325
00:20:03,670 --> 00:20:06,050
What are we trying to do with this sum?

326
00:20:06,370 --> 00:20:08,833
Well, it turns out we're
trying to take these

327
00:20:08,834 --> 00:20:10,990
exponential values,
which has this shape.

328
00:20:11,270 --> 00:20:12,270
It is a matrix.

329
00:20:13,000 --> 00:20:17,670
And we're attempting to divide it by what
we do the sum operation with.

330
00:20:18,645 --> 00:20:23,090
And we don't actually want to just
willy-nilly divide it.

331
00:20:23,430 --> 00:20:25,086
It's really important that things line up.

332
00:20:25,110 --> 00:20:30,150
So, it's for the same reason that we want
to make sure values are lining up when we

333
00:20:30,151 --> 00:20:33,710
do the dot product, and when we're doing
like a matrix product, right?

334
00:20:34,265 --> 00:20:37,890
So, even though NP dot dot, it is a dot
product with vectors, but it's also doing

335
00:20:37,891 --> 00:20:40,710
a matrix product for us, we need those
values to line up.

336
00:20:40,750 --> 00:20:42,510
So, we do a transpose, right?

337
00:20:44,000 --> 00:20:47,290
For that same reason, we need the right
values to line up here.

338
00:20:47,565 --> 00:20:54,050
Right now, if we did this, it's not going
to actually be dividing what we want

339
00:20:54,051 --> 00:20:58,070
exponentials to be divided by,
right?

340
00:20:58,390 --> 00:21:02,130
So, what we want to do is we need to shape
this correctly.

341
00:21:03,050 --> 00:21:08,810
And we could reshape this, but we can also
just simply use keep-dems.

342
00:21:08,910 --> 00:21:11,790
So, keep-is it keep-dems?

343
00:21:12,210 --> 00:21:12,830
Yeah, all one word.

344
00:21:13,070 --> 00:21:15,150
And then true, run that.

345
00:21:15,690 --> 00:21:19,704
And now, it is a matrix
of the exact same- I

346
00:21:19,705 --> 00:21:24,590
hate to say shape, but
it's the same orientation.

347
00:21:25,140 --> 00:21:26,380
It's the same dimensions, okay?

348
00:21:27,010 --> 00:21:28,530
So, it's just a sum.

349
00:21:28,790 --> 00:21:31,010
So now, it is literally this right here.

350
00:21:31,150 --> 00:21:33,380
It's just the sum of
these values, the sum of

351
00:21:33,381 --> 00:21:35,971
these values, and then
the sum of these values.

352
00:21:36,730 --> 00:21:43,930
So, with that, what we can do is we can
say the norm values is exponential values

353
00:21:44,470 --> 00:21:53,471
divided by the sum of exponential values at
axes one, and then keep-dems equals true.

354
00:21:57,340 --> 00:22:05,190
We get rid of this row here, and now we
have print norm values.

355
00:22:05,910 --> 00:22:10,190
We run that, and now we have our actual
normalized values.

356
00:22:12,310 --> 00:22:19,610
So, from this point, we really have one
more thing, and I think we'll cover it

357
00:22:20,910 --> 00:22:26,128
here, is one problem
that we have up to this

358
00:22:26,129 --> 00:22:30,390
point is with specifically
exponential values.

359
00:22:31,110 --> 00:22:34,086
So, there's one thing that we
want to change with exponential

360
00:22:34,087 --> 00:22:37,350
values before we convert
this to our actual class object.

361
00:22:37,575 --> 00:22:41,964
So, with that, let us
go ahead and talk about

362
00:22:41,965 --> 00:22:45,251
what we're going to do
with exponential values.

363
00:22:46,185 --> 00:22:49,600
One slight issue with
exponentiation is the explosion of

364
00:22:49,601 --> 00:22:53,150
values as the input to the
exponential function grows.

365
00:22:54,270 --> 00:22:58,650
It doesn't take much to get massive
numbers, and even worse, it doesn't take

366
00:22:58,850 --> 00:23:00,690
too long to reach an overflow.

367
00:23:03,570 --> 00:23:08,150
So, one way to combat this overflow is to
take all of the values in this output

368
00:23:08,151 --> 00:23:14,110
layer prior to exponentiation,
and subtract the largest value in that

369
00:23:14,111 --> 00:23:16,750
layer from all of the values in that
layer.

370
00:23:17,480 --> 00:23:20,465
And what this causes is
now the largest value will be a

371
00:23:20,466 --> 00:23:23,850
zero, and everything else
is going to be less than zero.

372
00:23:24,550 --> 00:23:28,630
Now, because the largest value prior to
exponentiation is actually a zero,

373
00:23:29,500 --> 00:23:37,350
our range of possibilities becomes
somewhere between zero and one after

374
00:23:37,351 --> 00:23:41,150
exponentiation, because the exponentiation
of zero equals one.

375
00:23:41,960 --> 00:23:45,310
So, this means our range of
values can only be between

376
00:23:45,311 --> 00:23:47,910
zero and one, thus no more
worrying about overflowing.

377
00:23:47,911 --> 00:23:53,250
So, the final concern that you might have
is what sort of impact does subtracting

378
00:23:53,251 --> 00:23:57,020
the max value from
everything have on the actual

379
00:23:57,021 --> 00:23:59,970
output of the softmax
activation function.

380
00:24:00,350 --> 00:24:04,790
So, all other things being equal and
assuming that we don't have some sort of

381
00:24:05,090 --> 00:24:10,670
overflow error, if we have two output
layers and one we don't subtract the max

382
00:24:10,671 --> 00:24:15,730
from and one we do, after we do our
exponentiation and our normalization,

383
00:24:16,030 --> 00:24:18,810
the actual output is identically the same.

384
00:24:18,970 --> 00:24:23,490
The only thing we've done is protected
ourselves from an actual overflow error.

385
00:24:23,770 --> 00:24:29,470
Alright, so with the subtraction of the
max value in mind, we are going to now go

386
00:24:29,471 --> 00:24:33,884
to the code that we left
off on with part five and

387
00:24:33,885 --> 00:24:37,490
we're going to add our
softmax activation class.

388
00:24:38,230 --> 00:24:43,310
And then we're going to implement it here
as well as change things here.

389
00:24:43,510 --> 00:24:46,590
I've used different variable names and
stuff like that.

390
00:24:46,930 --> 00:24:48,290
So, I'm going to
attempt to convert this to

391
00:24:48,291 --> 00:24:51,131
be exactly what we
have in the book as well.

392
00:24:51,370 --> 00:24:53,394
And so, by doing that
we'll also be able to

393
00:24:53,395 --> 00:24:56,411
finally test the softmax
activation function.

394
00:24:56,510 --> 00:24:58,850
So, these are the changes that we want to
make.

395
00:24:58,950 --> 00:25:03,750
So, first off, we're going to be
redefining this data.

396
00:25:03,990 --> 00:25:06,310
So, I'm actually going to delete these.

397
00:25:09,930 --> 00:25:14,450
Then, we'll just add this underneath the
value activation function.

398
00:25:14,810 --> 00:25:20,850
And we're going to say class activation,
activation underscore softmax.

399
00:25:21,890 --> 00:25:27,450
And then we're going to define the forward
method, again, self-inputs.

400
00:25:28,670 --> 00:25:32,010
And now we're just going to implement the
code that we've talked about and covered

401
00:25:32,011 --> 00:25:33,746
already up to this
point, except for the max

402
00:25:33,747 --> 00:25:35,710
thing, but I'm going to
show you that in a moment.

403
00:25:36,210 --> 00:25:42,510
So, the exponential underscore values is
going to be equal to np dot x of inputs.

404
00:25:43,030 --> 00:25:45,290
And then we want to minus the np dot max.

405
00:25:47,010 --> 00:25:49,450
And immediately alarm bells should be sort
of going off.

406
00:25:49,810 --> 00:25:52,837
And that is, if we run
np dot max, and we, for

407
00:25:52,838 --> 00:25:55,490
example, we're going
to say np dot max inputs.

408
00:25:55,491 --> 00:25:59,970
And also, while we're here, what is
inputs?

409
00:26:00,390 --> 00:26:04,610
So, up to this point, inputs, in this
case, because we're using the softmax

410
00:26:04,611 --> 00:26:09,490
activation function for the activation
function of an output layer, inputs are

411
00:26:09,491 --> 00:26:13,250
actually the outputs so far of our model,
right?

412
00:26:14,050 --> 00:26:17,544
And so, again, we've
got these outputs, and

413
00:26:17,545 --> 00:26:22,051
those outputs are
going to be in batch form.

414
00:26:22,110 --> 00:26:23,870
So, there will be a batch of these
outputs.

415
00:26:23,871 --> 00:26:29,790
So, if we just said np dot max of inputs,
which would be np dot max of a batch of

416
00:26:29,791 --> 00:26:35,551
outputs, up to this point, it's going to be
the highest value of all of those values.

417
00:26:35,580 --> 00:26:37,630
Now, you'd probably get away with it.

418
00:26:41,990 --> 00:26:44,266
I think it would probably work,
but it would be a mistake.

419
00:26:44,290 --> 00:26:46,710
It would work, but not as well as it
should.

420
00:26:47,210 --> 00:26:53,130
So, instead of what you want to say is np
dot max inputs of, and immediately you

421
00:26:53,131 --> 00:26:55,752
should already know where
we're headed, it's going to

422
00:26:55,753 --> 00:26:58,930
be of axes one, and then,
again, keep dems will be true.

423
00:26:59,960 --> 00:27:02,880
So, we're going to take inputs,
which is going to be this batch of inputs.

424
00:27:03,040 --> 00:27:06,785
And, again, we want to subtract
the max along of axes one,

425
00:27:06,786 --> 00:27:12,710
and then we want to keep that
dimension, right, of that array.

426
00:27:15,710 --> 00:27:20,190
So, once we've done that, we now have our
exponential values of a batch,

427
00:27:20,625 --> 00:27:24,530
and we are subtracting the max along the
way.

428
00:27:24,870 --> 00:27:27,350
Again, so we just don't hit an overflow
value.

429
00:27:27,550 --> 00:27:33,170
The actual outputs will be exactly the
same after we normalize all that.

430
00:27:33,330 --> 00:27:35,690
So, we're not really losing anything by
doing this.

431
00:27:35,810 --> 00:27:36,890
We're just preventing an overflow.

432
00:27:36,891 --> 00:27:39,650
So, exponential values is done.

433
00:27:40,090 --> 00:27:48,090
Now we're going to say probability is
going to be equal to x values divided by

434
00:27:50,585 --> 00:27:54,843
np dot sum of the
exponential values of axes

435
00:27:54,844 --> 00:27:58,130
one, axes one, keep
dems is going to be true.

436
00:27:58,530 --> 00:28:01,530
And, again, we've already actually... this
we've already covered, but again,

437
00:28:01,531 --> 00:28:07,970
we're trying to sum each of the rows,
and we want to keep the dimension.

438
00:28:09,170 --> 00:28:11,973
So, finally, we're going
to define our self dot

439
00:28:11,974 --> 00:28:14,791
output, and that is going
to be the probabilities.

440
00:28:16,930 --> 00:28:17,370
Okay.

441
00:28:17,720 --> 00:28:21,050
So, that's our soft max activation
function, and now what I want to make sure

442
00:28:21,051 --> 00:28:25,070
I do is I use the exact same layer values
and so on so.

443
00:28:25,910 --> 00:28:28,750
We already got rid of the code up above,
I think.

444
00:28:31,855 --> 00:28:36,130
I think what I'll do... I think we'll just
code it from scratch here.

445
00:28:36,535 --> 00:28:38,470
So, here is our framework up to this
point.

446
00:28:38,870 --> 00:28:40,446
Well, first thing we're going to do is
define our data.

447
00:28:40,470 --> 00:28:45,430
So, x, y is going to be equal to spiral
data, and then we're going to say samples.

448
00:28:45,920 --> 00:28:47,266
How many samples do we want to have?

449
00:28:47,290 --> 00:28:50,150
We're going to say 100 per class,
and we're going to say classes.

450
00:28:50,900 --> 00:28:52,580
We're going to have three separate
classes.

451
00:28:54,890 --> 00:29:00,230
Now, what we're going to do is we're going
to say dense one is equal to layer,

452
00:29:00,231 --> 00:29:05,370
dense, and the shape here is going to be
two, and then three.

453
00:29:05,650 --> 00:29:07,190
So, we only have two input features.

454
00:29:07,650 --> 00:29:10,710
Again, spiral data is just x, y data,
so it's just coordinates.

455
00:29:11,050 --> 00:29:15,710
So, the input here is going to be x,
y, that's two, so the input must be two.

456
00:29:16,230 --> 00:29:17,666
Now, the output could be anything you
want.

457
00:29:17,690 --> 00:29:20,058
You could say 90 if you wanted,
but we're going to say three

458
00:29:20,059 --> 00:29:22,010
because we're going to
follow, we're going to say three.

459
00:29:22,011 --> 00:29:23,610
Because we're going to follow the book.

460
00:29:24,180 --> 00:29:31,170
Now, after we've got dense one,
we're going to define also activation one,

461
00:29:31,620 --> 00:29:34,970
and that will be an activation value.

462
00:29:37,180 --> 00:29:41,490
And then we're going to define dense two
is going to be another layer dense.

463
00:29:42,030 --> 00:29:44,875
It, the input, because
the output of the previous

464
00:29:44,876 --> 00:29:48,430
layer was a three, the
input must be three.

465
00:29:48,431 --> 00:29:50,407
Now, the output could
be anything we want, but

466
00:29:50,408 --> 00:29:53,311
we're going to treat this
like it's an output layer.

467
00:29:53,770 --> 00:29:57,030
So, how many neurons should this output
layer have?

468
00:29:57,370 --> 00:30:00,510
Well, we have three classes, so we're
going to say three.

469
00:30:01,340 --> 00:30:05,384
So, dense two is defined,
and then we're going to say

470
00:30:05,385 --> 00:30:10,410
activation, activation two is
going to be an activation softmax.

471
00:30:11,610 --> 00:30:15,110
And then now, we're going to begin passing
data actually through here.

472
00:30:15,230 --> 00:30:20,110
So, we're going to say dense one,
dense one dot forward, and we're going to

473
00:30:20,260 --> 00:30:22,850
forward our actual input data here.

474
00:30:23,770 --> 00:30:25,390
Then, we need to activate it.

475
00:30:25,610 --> 00:30:31,850
So, activation one dot forward,
dense one dot output.

476
00:30:31,950 --> 00:30:35,870
Then, we're going to pass that through
dense two.

477
00:30:36,130 --> 00:30:43,570
So, we're going to say dense two dot
forward, activation one dot output.

478
00:30:44,840 --> 00:30:50,950
And then, activation two dot forward the
output from dense two.

479
00:30:53,270 --> 00:30:55,907
And, at this point, because
we do the forward, we now

480
00:30:55,908 --> 00:30:58,071
have the output, which is
going to be probabilities.

481
00:30:58,465 --> 00:31:03,970
So, for example, we can print activation
two dot output, and let's just do the,

482
00:31:04,090 --> 00:31:05,970
there should be three hundred of them.

483
00:31:06,560 --> 00:31:08,690
So, we're going to just do the first five.

484
00:31:09,770 --> 00:31:10,890
Let's go ahead and run that.

485
00:31:11,550 --> 00:31:15,710
And, what we get is, again, this is a
batch.

486
00:31:16,470 --> 00:31:19,010
Okay, so we passed all of them at the same
time, in this case.

487
00:31:19,540 --> 00:31:22,090
So, we, theory, had a batch of three
hundred.

488
00:31:22,570 --> 00:31:23,870
Now, each of these is unique.

489
00:31:24,620 --> 00:31:26,310
So, this is a sample.

490
00:31:27,150 --> 00:31:29,804
So, this had an input
of two values, and this

491
00:31:29,805 --> 00:31:31,930
is each neuron value
of those two values.

492
00:31:32,510 --> 00:31:35,470
And, it just continues to repeat,
and then this is just the first five.

493
00:31:36,155 --> 00:31:37,326
Now, it should be no surprise.

494
00:31:37,350 --> 00:31:38,850
Everything was initialized randomly.

495
00:31:39,650 --> 00:31:45,910
So, it turns out that the distribution of
predictions is indeed random.

496
00:31:46,170 --> 00:31:49,570
It's a perfect, you know, not a perfect,
but very close to a perfect one-third

497
00:31:49,970 --> 00:31:50,830
prediction for everything.

498
00:31:50,831 --> 00:31:51,950
This is totally normal.

499
00:31:52,790 --> 00:31:56,010
When you randomly initialize a model,
that's kind of what you expect.

500
00:31:58,410 --> 00:32:03,130
So, now what we need to do is actually
train this model.

501
00:32:03,700 --> 00:32:05,632
Because, like, for
example, let's see if any

502
00:32:05,633 --> 00:32:08,791
of these are obviously
bigger than the others.

503
00:32:09,230 --> 00:32:10,610
Three, three, three, five.

504
00:32:10,910 --> 00:32:18,190
So, in this case, right, if we did an
argmax on this right here, this class,

505
00:32:18,370 --> 00:32:24,630
you know, of index two, is the largest,
unless I'm missing something, but anyways,

506
00:32:25,150 --> 00:32:26,430
it is the largest it looks like.

507
00:32:26,750 --> 00:32:30,770
So, that would be the prediction,
and maybe that would be accurate.

508
00:32:30,970 --> 00:32:32,026
Maybe that would be correct.

509
00:32:32,050 --> 00:32:34,051
But instead, what we
want to really do is we

510
00:32:34,052 --> 00:32:36,170
don't want to know just
what's right and wrong.

511
00:32:36,560 --> 00:32:39,241
We want to know how
right and how wrong, and

512
00:32:39,242 --> 00:32:42,451
to calculate that, we
use a loss function.

513
00:32:42,695 --> 00:32:45,310
And that is the topic of the next video.

514
00:32:45,890 --> 00:32:48,742
So, if you've got any
questions, comments, concerns,

515
00:32:48,743 --> 00:32:51,230
whatever, up to this point,
we kind of covered a lot.

516
00:32:51,450 --> 00:32:55,790
But I think the Softmax activation function
is relatively simplistic to pick up.

517
00:32:56,330 --> 00:32:58,540
But if you have questions,
comments, concerns,

518
00:32:58,541 --> 00:33:00,230
whatever, feel free
to leave those below.

519
00:33:00,770 --> 00:33:04,510
Again, the physical versions of the books
are done.

520
00:33:05,330 --> 00:33:07,630
As are obviously the e-book and all that.

521
00:33:07,940 --> 00:33:09,770
It's ready to be downloaded and consumed.

522
00:33:10,570 --> 00:33:13,690
So, if you want something to follow along
with the videos, or you want to kind of

523
00:33:13,691 --> 00:33:17,750
refresh from the videos, or read before
the video, watch the video, that kind of

524
00:33:17,751 --> 00:33:21,150
stuff, definitely check that out at nnfs
.io.

525
00:33:22,525 --> 00:33:25,030
Otherwise, I will see you guys in another
video.

