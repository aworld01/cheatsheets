{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f202c68-6a7e-45ce-8fbd-26c925f6c24f",
   "metadata": {},
   "source": [
    "<h1>1:00:00/1:23:15</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f45794-8bdf-4418-8b9a-d2c73e78402d",
   "metadata": {},
   "source": [
    "<h1>Workflow</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd49570-f7b3-4e5d-ada0-6c0d5b448056",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    0. Import important libraries<br>\n",
    "    1. Get dataset ready (turn into tensor and batches)<br>\n",
    "    2. Build a NeuralNetwork model for classification<br>\n",
    "    3. Pick a loss function and optimizer<br>\n",
    "    4. Build a training loop<br>\n",
    "    5. Evaluate your model<br>\n",
    "    6. Improve your model<br>\n",
    "    7. Save the model\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f0b7e-5270-427e-b150-0574d7344094",
   "metadata": {},
   "source": [
    "<h2>0. Import important libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63d2a9c-904b-4020-a408-065b5198f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dec84e-5a64-4a1f-9d70-a7a41dfb934e",
   "metadata": {},
   "source": [
    "<h2>1. Get dataset ready (turn into tensor and batches)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd435765-2141-4d17-8b5c-7038c59efd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.FashionMNIST(root='/dataset',train=True,transform=transforms.ToTensor(),download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='/dataset',train=False,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a7dd70-4be5-4566-b3c7-12a801fe60ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /dataset\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset #to check dataset items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e51e47f-8209-4cbc-9914-b6f6d5edae11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: /dataset\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset #to check dataset items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06bd1589-be7b-4194-8ef2-5b35f7cf6c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83527e-3bb2-4e10-a207-fdd68be4c535",
   "metadata": {},
   "source": [
    "<h3>1.2 Conveting data into batches</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719061ae-4852-4e2f-be74-32be1a952b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d20313e0-5275-4d52-a9d2-9361bee598fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader)) #60000 / 64 = 938\n",
    "print(len(test_loader)) #10000 / 64 = 157"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb0069-010f-4e05-9ac5-61ea13fc1918",
   "metadata": {},
   "source": [
    "<h2>2. Build a Neural Network model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53db2484-bc27-4fdf-80e7-4f578e514523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,in_dim,n_hidden_1,n_hidden_2,out_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_dim,n_hidden_1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(n_hidden_1,n_hidden_2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(n_hidden_2,out_dim),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        a = self.layer1(x)\n",
    "        b = self.layer2(a)\n",
    "        c = self.layer3(b)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "338a444f-a89f-4add-831f-88443d01919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 28*28 #784\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 100\n",
    "out_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c265ce-10ca-4130-8b92-348cd6df192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(in_dim,n_hidden_1,n_hidden_2,out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4edc4607-5a10-4b22-830a-92f7171239ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=300, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=10, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "317f5612-d6b4-4f70-af5e-882765ea81be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.0.weight',\n",
       "              tensor([[ 0.0272, -0.0327,  0.0027,  ..., -0.0140, -0.0212,  0.0312],\n",
       "                      [-0.0258,  0.0203,  0.0059,  ..., -0.0032, -0.0163,  0.0279],\n",
       "                      [-0.0131,  0.0104,  0.0196,  ..., -0.0127, -0.0350, -0.0233],\n",
       "                      ...,\n",
       "                      [-0.0121,  0.0223,  0.0295,  ..., -0.0230,  0.0130,  0.0163],\n",
       "                      [ 0.0283, -0.0021,  0.0215,  ...,  0.0067, -0.0034, -0.0139],\n",
       "                      [-0.0146, -0.0163,  0.0055,  ...,  0.0068, -0.0274,  0.0061]])),\n",
       "             ('layer1.0.bias',\n",
       "              tensor([ 2.5298e-02, -1.2053e-02,  2.7205e-02,  4.1494e-04, -3.5277e-02,\n",
       "                       3.4247e-02,  3.5429e-02,  1.8444e-02, -1.5819e-02, -3.1231e-02,\n",
       "                      -3.1024e-02, -1.5068e-02,  2.8809e-02, -4.8940e-03, -1.8728e-02,\n",
       "                       3.3695e-02,  1.3039e-02,  1.7287e-02, -1.2008e-02,  2.3243e-02,\n",
       "                      -9.6171e-04,  2.3277e-02,  2.7814e-02,  2.1668e-02, -3.5346e-02,\n",
       "                       1.6285e-02,  1.4131e-02,  1.3607e-02,  1.1203e-02, -2.8106e-02,\n",
       "                      -2.8241e-02, -2.1301e-02, -2.8933e-02, -1.5350e-02,  3.1939e-02,\n",
       "                       3.3493e-02, -2.1167e-02, -3.2826e-02, -1.4009e-02,  1.1139e-02,\n",
       "                      -2.8660e-02, -1.2132e-02,  9.5809e-03, -2.3716e-02, -2.0286e-02,\n",
       "                       2.8845e-02,  9.3338e-03,  2.6975e-02,  3.5707e-02, -1.3258e-03,\n",
       "                      -7.5358e-03, -2.6385e-02,  3.1888e-02, -1.4156e-02, -4.1869e-04,\n",
       "                      -1.1515e-02,  2.1892e-02, -1.6217e-02,  4.9490e-03,  1.7114e-02,\n",
       "                      -4.7756e-03,  1.2739e-02,  7.8070e-03, -9.1475e-03, -1.5302e-02,\n",
       "                       1.9760e-02, -3.0129e-02,  2.5749e-02, -2.8709e-03,  3.8308e-03,\n",
       "                       3.0038e-02,  2.2708e-02, -2.5509e-02,  3.1645e-02, -7.6997e-03,\n",
       "                       1.4460e-03,  1.7460e-02, -4.6841e-03,  6.0722e-03,  2.0246e-02,\n",
       "                       3.1890e-02,  1.7290e-02, -2.2898e-02,  2.0678e-02, -1.6897e-02,\n",
       "                       2.1309e-02, -1.1530e-02, -1.6962e-02, -3.2671e-03, -1.7806e-02,\n",
       "                      -3.0076e-02, -2.5992e-02, -9.2434e-03,  2.8090e-02,  2.2382e-02,\n",
       "                       2.1130e-02,  1.3021e-02,  6.5220e-03,  1.9790e-02,  2.0160e-02,\n",
       "                      -2.6390e-04,  2.1450e-02,  1.2913e-02, -2.9015e-02,  2.7969e-02,\n",
       "                       2.3195e-02, -1.0250e-02, -1.1246e-02, -9.3631e-03,  2.8565e-02,\n",
       "                       2.1699e-02,  1.7477e-02, -9.6490e-03, -2.5580e-02, -6.3208e-03,\n",
       "                      -2.8413e-02, -2.2953e-02,  1.8608e-02, -5.4128e-03,  3.4666e-02,\n",
       "                      -2.8002e-02,  4.6190e-03,  3.1910e-03,  3.5149e-03,  6.6214e-04,\n",
       "                      -1.6621e-02, -2.9664e-02, -1.8942e-02, -2.8799e-03, -1.0918e-02,\n",
       "                       1.4769e-02, -2.8864e-02, -3.3568e-02, -2.7818e-02,  3.1035e-02,\n",
       "                       3.8370e-03, -6.6434e-03,  1.1112e-02,  7.4316e-03,  2.3170e-02,\n",
       "                      -4.1019e-05, -2.6309e-02,  1.0168e-02, -1.2903e-02, -1.8039e-02,\n",
       "                      -3.2636e-02, -3.7706e-03,  3.1553e-02, -2.4297e-02, -3.2507e-02,\n",
       "                      -3.1518e-02, -2.0397e-02,  3.1876e-02,  1.6753e-02,  2.1195e-03,\n",
       "                      -9.9583e-03, -1.8490e-02,  2.0479e-02, -6.4404e-04, -7.7497e-05,\n",
       "                       4.7446e-04,  3.4764e-02, -2.7085e-02,  2.7630e-02,  2.6400e-02,\n",
       "                       2.2975e-02,  1.3430e-02, -5.3561e-04,  1.1827e-02, -7.5142e-03,\n",
       "                      -2.8409e-02,  3.3188e-02,  3.0545e-02, -3.1643e-02,  1.6438e-02,\n",
       "                      -1.1615e-02,  8.8106e-04,  1.2323e-02, -1.7371e-02, -3.3284e-02,\n",
       "                      -1.7459e-02, -1.1072e-02, -3.3808e-02, -3.1784e-02,  4.9351e-03,\n",
       "                       2.6848e-02,  1.1131e-02, -3.0523e-02, -8.5871e-03,  1.6786e-02,\n",
       "                       3.1626e-02,  1.7599e-02, -3.0361e-02,  7.7715e-03,  8.5808e-03,\n",
       "                      -1.9052e-02, -2.1060e-02,  3.2914e-02,  2.8377e-02,  3.0057e-02,\n",
       "                      -1.1949e-02,  7.6487e-03, -3.4116e-02, -1.6799e-02, -1.7121e-02,\n",
       "                       2.4107e-02, -5.4999e-03,  2.3728e-02, -1.4132e-02, -1.0462e-02,\n",
       "                       1.9802e-02, -1.9165e-02,  2.3551e-02, -2.9595e-02, -2.2518e-02,\n",
       "                       1.1425e-02, -3.5227e-02, -1.9464e-03, -2.4156e-02, -1.5203e-02,\n",
       "                       2.4706e-02,  2.6545e-02, -1.8883e-03, -1.5741e-02,  2.8806e-02,\n",
       "                       2.2344e-02,  2.3148e-02, -2.5644e-02, -3.2097e-02, -1.3626e-02,\n",
       "                      -1.9954e-02, -1.8460e-02,  3.0005e-02,  1.0895e-02, -2.4533e-02,\n",
       "                       8.7612e-03, -1.8798e-02,  1.7559e-02,  6.8141e-03, -3.0116e-02,\n",
       "                       9.6229e-03, -1.5399e-02,  2.5703e-02, -1.0786e-02,  6.1055e-04,\n",
       "                       1.7576e-02, -2.3984e-02, -3.0822e-02, -1.2034e-02,  3.3998e-02,\n",
       "                       2.1285e-02, -4.7380e-03, -7.2377e-03, -2.3118e-02,  8.7018e-03,\n",
       "                      -2.6506e-02, -1.8154e-02, -1.4962e-02,  1.1278e-02,  5.0279e-03,\n",
       "                       1.7570e-02, -2.3345e-02, -1.7920e-02, -1.1826e-02, -2.2456e-02,\n",
       "                       3.0812e-02,  4.7834e-03, -1.8979e-02, -2.6092e-02,  2.5981e-02,\n",
       "                      -1.9088e-02, -1.8385e-02, -1.6582e-02,  1.3248e-02,  1.9095e-02,\n",
       "                      -1.9554e-03,  2.7661e-02, -2.3560e-02,  5.8660e-03,  1.0046e-02,\n",
       "                       3.1665e-02, -4.2347e-04,  1.4173e-02, -1.8492e-02,  1.6904e-02,\n",
       "                       2.0898e-02, -5.8149e-03, -2.0578e-02,  3.4238e-02,  7.8184e-03,\n",
       "                      -9.5455e-03,  2.2113e-02, -1.2983e-02, -2.8124e-02, -2.4959e-02,\n",
       "                      -2.9202e-03,  3.0875e-02, -1.2508e-02, -2.7697e-03, -1.2398e-02])),\n",
       "             ('layer2.0.weight',\n",
       "              tensor([[-0.0351,  0.0428, -0.0178,  ..., -0.0153, -0.0127,  0.0529],\n",
       "                      [ 0.0186,  0.0556,  0.0577,  ..., -0.0347, -0.0275, -0.0555],\n",
       "                      [ 0.0338,  0.0556, -0.0013,  ...,  0.0038, -0.0430, -0.0436],\n",
       "                      ...,\n",
       "                      [-0.0484,  0.0225, -0.0329,  ...,  0.0509,  0.0492, -0.0478],\n",
       "                      [ 0.0399, -0.0385, -0.0072,  ..., -0.0034, -0.0261,  0.0568],\n",
       "                      [-0.0002,  0.0282, -0.0157,  ..., -0.0325,  0.0444,  0.0367]])),\n",
       "             ('layer2.0.bias',\n",
       "              tensor([-0.0066, -0.0535, -0.0232,  0.0104, -0.0551,  0.0406, -0.0475,  0.0223,\n",
       "                       0.0287, -0.0480, -0.0102,  0.0036, -0.0037,  0.0466,  0.0470, -0.0389,\n",
       "                       0.0487, -0.0239,  0.0066, -0.0056, -0.0085, -0.0427,  0.0019,  0.0218,\n",
       "                      -0.0147,  0.0407,  0.0453, -0.0487, -0.0151, -0.0198,  0.0207,  0.0434,\n",
       "                      -0.0130, -0.0125,  0.0379, -0.0238,  0.0136, -0.0002, -0.0424, -0.0236,\n",
       "                      -0.0411,  0.0497, -0.0308, -0.0525, -0.0013, -0.0209,  0.0189,  0.0077,\n",
       "                       0.0546,  0.0315,  0.0362, -0.0454, -0.0238, -0.0435, -0.0502,  0.0167,\n",
       "                      -0.0077, -0.0449,  0.0076, -0.0267, -0.0546,  0.0459,  0.0424, -0.0204,\n",
       "                       0.0453,  0.0323,  0.0090,  0.0015, -0.0224, -0.0295, -0.0366, -0.0393,\n",
       "                      -0.0100,  0.0049,  0.0397, -0.0227, -0.0328,  0.0176,  0.0549,  0.0271,\n",
       "                       0.0083,  0.0384, -0.0295, -0.0104,  0.0020,  0.0142, -0.0269,  0.0385,\n",
       "                      -0.0233,  0.0506,  0.0281,  0.0412,  0.0568, -0.0575,  0.0080,  0.0001,\n",
       "                       0.0501,  0.0295,  0.0119,  0.0216])),\n",
       "             ('layer3.0.weight',\n",
       "              tensor([[-9.0655e-02,  2.6859e-02,  2.5875e-02, -6.5044e-04,  1.1059e-04,\n",
       "                        6.6259e-03, -7.6853e-02, -1.2415e-02,  3.2008e-03, -7.5429e-02,\n",
       "                       -1.9077e-02, -7.4286e-02, -9.7784e-02, -8.9616e-02, -5.7523e-02,\n",
       "                        2.7028e-02, -8.3802e-03, -5.6775e-02, -3.6941e-02, -9.3869e-02,\n",
       "                       -9.7248e-02,  9.3332e-03,  2.8516e-02,  8.1399e-02, -8.9606e-02,\n",
       "                        7.6543e-02, -8.1155e-02, -3.2672e-02,  4.4137e-02,  9.1678e-02,\n",
       "                        6.1127e-02,  2.7183e-03, -1.7632e-02, -5.3088e-02, -4.1806e-02,\n",
       "                       -3.6286e-02, -7.7008e-02, -5.5014e-02, -2.2508e-02,  5.4099e-02,\n",
       "                        3.8900e-02,  5.8346e-02,  4.9581e-02, -5.6854e-02,  8.5123e-02,\n",
       "                       -6.5869e-02, -5.1209e-04, -1.7649e-02,  8.0655e-02, -9.0879e-02,\n",
       "                        5.7842e-02, -7.2302e-02,  3.6537e-02, -1.8573e-02,  6.2899e-02,\n",
       "                        1.1678e-02, -7.1320e-02,  6.8818e-02,  5.7260e-02, -3.0122e-02,\n",
       "                       -4.0839e-02,  2.4447e-02,  7.9037e-02, -1.7668e-03,  3.7760e-03,\n",
       "                        2.6434e-02,  9.1062e-02, -9.5862e-02, -1.3668e-02,  4.7377e-03,\n",
       "                       -3.9158e-02,  2.9735e-02,  8.8100e-02,  2.7458e-02, -5.0512e-03,\n",
       "                       -9.7109e-02,  6.3001e-02,  1.3690e-02, -2.4733e-02, -6.1807e-02,\n",
       "                       -4.7164e-02,  3.1135e-02, -1.9480e-02, -6.7613e-02, -6.3693e-02,\n",
       "                        6.6466e-02,  8.4335e-02,  8.5461e-02,  4.7912e-02,  9.3278e-02,\n",
       "                       -7.3083e-02, -5.4599e-02, -1.3190e-02,  1.5895e-02,  4.7687e-02,\n",
       "                        8.8672e-03,  7.5216e-02, -7.9867e-02,  6.6087e-02, -8.5237e-02],\n",
       "                      [ 6.2518e-02, -2.5488e-02,  5.4319e-02,  6.7956e-02, -8.5738e-02,\n",
       "                       -8.1330e-02, -6.3325e-02,  9.2157e-02, -3.0094e-02,  2.5007e-02,\n",
       "                       -8.1034e-02, -3.9892e-02, -4.9801e-03,  2.9503e-02,  8.1871e-02,\n",
       "                        2.1061e-02, -1.7771e-02,  7.2186e-02, -5.8567e-02,  6.7087e-02,\n",
       "                        1.7991e-03,  2.2580e-02,  6.3448e-02, -7.7165e-02, -8.9708e-02,\n",
       "                        8.0098e-02, -6.7955e-02,  2.0582e-02,  4.7505e-02,  4.2458e-02,\n",
       "                       -6.3249e-02,  8.6066e-02, -9.5200e-02,  4.1098e-02,  8.2674e-02,\n",
       "                       -7.8555e-02, -9.1811e-02,  2.0847e-02,  8.0311e-02, -7.9320e-02,\n",
       "                       -5.8686e-02,  2.6356e-02, -1.7071e-02,  9.4051e-02,  5.5220e-03,\n",
       "                       -2.0514e-03, -8.7122e-02, -4.4152e-02, -6.3425e-04,  1.4225e-02,\n",
       "                        3.2239e-02, -6.3859e-02, -6.3972e-02, -1.9901e-02,  3.4553e-02,\n",
       "                        9.9233e-02, -6.2880e-02, -6.7171e-02, -2.6628e-03, -8.5606e-02,\n",
       "                        5.0150e-02,  9.2640e-02, -1.8032e-02,  9.9596e-02,  6.8324e-02,\n",
       "                        6.6914e-02, -2.3898e-03,  2.1406e-02, -8.0875e-02, -6.9864e-02,\n",
       "                       -4.9049e-02, -9.8590e-02,  2.3526e-02, -3.7584e-02,  1.2998e-02,\n",
       "                       -1.9395e-02,  4.2230e-02,  6.9656e-02,  5.1929e-02,  3.4438e-02,\n",
       "                       -6.5293e-02,  1.5264e-02, -8.4411e-02,  9.6124e-02,  3.5550e-02,\n",
       "                        9.7375e-02,  8.4232e-02,  7.6280e-02, -7.1920e-05, -4.7027e-02,\n",
       "                       -4.1626e-02,  3.2979e-03, -3.3460e-02, -4.7819e-02,  8.6650e-02,\n",
       "                        9.7734e-02, -4.2789e-02, -1.4850e-02, -3.6155e-02, -8.5487e-02],\n",
       "                      [ 5.8710e-02, -1.2785e-02, -9.9457e-02, -2.6806e-02,  5.1483e-03,\n",
       "                       -8.6493e-02,  6.8326e-02, -4.8177e-02, -3.5062e-02, -7.4929e-02,\n",
       "                       -8.2108e-02, -1.1098e-02,  5.4670e-02,  4.9321e-02, -2.3407e-02,\n",
       "                       -8.6633e-02,  8.1477e-02, -5.0894e-02,  1.4509e-02,  8.0003e-02,\n",
       "                        9.8916e-03,  7.3825e-02, -5.3076e-02, -9.1259e-02, -7.1836e-02,\n",
       "                        8.3906e-02,  1.1996e-02, -7.2598e-02,  9.6221e-02,  9.3076e-02,\n",
       "                        8.1172e-02, -8.4205e-02,  4.5835e-02, -3.9398e-02,  3.5265e-02,\n",
       "                       -9.4357e-03,  1.3370e-02, -1.3242e-02,  5.7042e-02, -6.4008e-02,\n",
       "                        6.4958e-03,  9.3121e-02,  3.1692e-02,  7.6903e-03,  2.1819e-02,\n",
       "                        8.6764e-02,  5.5002e-02, -5.7390e-02,  6.2826e-02,  6.4998e-02,\n",
       "                       -3.0247e-02,  2.7364e-02, -7.3348e-02,  9.6417e-02, -7.4373e-02,\n",
       "                       -6.1457e-02,  9.8609e-02, -9.9986e-02, -8.2018e-02, -2.0428e-02,\n",
       "                       -8.0759e-02,  8.9223e-02, -3.6376e-02,  4.8237e-02,  2.6170e-03,\n",
       "                       -7.7347e-02, -6.1361e-02, -4.4754e-02, -4.8754e-02,  7.5040e-02,\n",
       "                       -5.5327e-03, -1.1726e-02, -4.6257e-02, -7.6387e-02,  5.1190e-02,\n",
       "                        5.7488e-03, -1.2205e-02, -1.2658e-02,  9.1368e-02, -9.7390e-02,\n",
       "                        2.2762e-02, -9.4535e-02,  3.7107e-02, -4.4139e-02, -7.6786e-02,\n",
       "                        4.7010e-02,  2.4898e-02,  5.5076e-02,  6.8388e-02,  9.0615e-02,\n",
       "                        7.1008e-02, -9.6495e-02, -2.4599e-02,  6.2344e-02,  8.2326e-03,\n",
       "                        2.2565e-02, -3.0123e-02,  1.2371e-02, -6.2072e-02, -5.1860e-02],\n",
       "                      [-8.0272e-02,  7.4557e-02, -7.9677e-03,  5.2342e-03, -7.4073e-02,\n",
       "                        7.4363e-02, -5.7594e-02, -7.2277e-02, -7.1977e-02,  1.9604e-02,\n",
       "                        2.9649e-03,  1.7585e-02, -6.9155e-02, -5.7168e-02, -6.9353e-02,\n",
       "                        2.0081e-02,  2.2992e-02,  8.8137e-02,  6.4169e-03, -5.9719e-02,\n",
       "                       -2.4177e-03, -9.2557e-02, -5.2653e-02,  7.8195e-02,  2.2683e-02,\n",
       "                        3.8618e-02,  3.9442e-02,  9.5271e-03,  2.5127e-02,  5.3864e-02,\n",
       "                       -1.9577e-02,  5.9221e-02, -3.2100e-02, -8.6531e-02, -3.3710e-02,\n",
       "                        2.7615e-02, -8.0567e-02, -5.0406e-02,  7.4146e-02,  1.3683e-02,\n",
       "                        7.8023e-02,  6.0705e-02, -1.7010e-03,  9.0268e-02,  3.0705e-03,\n",
       "                       -2.0042e-02, -9.3573e-02,  7.8499e-02,  9.9966e-03, -2.2830e-02,\n",
       "                        7.3049e-03,  8.9387e-02, -3.9358e-02, -4.7341e-02,  4.7809e-02,\n",
       "                        6.7175e-02, -2.7942e-02, -3.1402e-02,  4.2028e-02, -8.4033e-02,\n",
       "                        7.5815e-02,  1.8044e-02, -1.7093e-02,  4.7796e-02, -2.9801e-02,\n",
       "                       -4.5146e-02,  5.3561e-02,  3.9453e-02, -9.5747e-02, -5.1857e-03,\n",
       "                        5.3965e-02,  6.2227e-02, -6.1041e-02, -3.3981e-02, -4.5166e-02,\n",
       "                       -3.8414e-02,  1.2017e-02,  4.1078e-02, -3.3923e-02,  2.4274e-02,\n",
       "                        7.7119e-02,  9.1790e-02,  3.7978e-02,  7.6326e-02, -2.3127e-02,\n",
       "                        9.1275e-02,  6.6693e-02, -3.9889e-02,  1.7052e-02, -2.1298e-02,\n",
       "                        7.4129e-02,  5.4438e-02,  5.2707e-04, -2.6399e-03, -7.5021e-02,\n",
       "                       -6.8400e-02,  1.3941e-03, -5.2180e-02,  4.3445e-02, -8.4770e-02],\n",
       "                      [-8.5906e-03,  8.6277e-02,  6.8733e-02, -9.8578e-02, -9.9026e-04,\n",
       "                       -5.9347e-02, -8.9606e-02,  5.4477e-02, -3.6566e-02,  4.3018e-02,\n",
       "                       -3.8772e-02, -4.5136e-02,  4.6547e-02,  3.1165e-02,  2.8553e-02,\n",
       "                        8.0238e-02, -2.3516e-02, -2.9390e-02,  6.2005e-02,  5.7907e-03,\n",
       "                       -6.1799e-02, -1.6683e-02,  6.3085e-02, -6.9148e-02, -5.6176e-02,\n",
       "                        6.5083e-02,  5.7329e-02, -5.3674e-02, -3.2929e-02,  7.0198e-02,\n",
       "                        6.6733e-02, -4.6611e-02, -9.2730e-02,  8.9677e-02, -7.9568e-02,\n",
       "                       -6.0792e-02,  5.8359e-02,  6.5143e-02, -4.1935e-02,  2.4172e-02,\n",
       "                       -8.7784e-02,  7.9494e-02, -3.7204e-02, -8.1586e-02,  3.5479e-02,\n",
       "                       -5.8940e-02,  9.2683e-02,  7.8897e-02, -9.3780e-02, -3.7691e-02,\n",
       "                        6.5979e-02, -5.0462e-02,  7.1176e-02, -8.0740e-02, -8.7005e-02,\n",
       "                        9.0598e-02,  6.7837e-02, -5.0805e-02, -6.7094e-02,  4.1277e-02,\n",
       "                       -3.4771e-02, -8.0568e-02,  4.1325e-02, -4.0396e-02,  4.6185e-02,\n",
       "                        9.6698e-02,  5.6279e-02, -4.6159e-03, -6.1414e-02,  7.5073e-03,\n",
       "                        7.7249e-02,  1.3314e-02, -3.5118e-02, -3.3422e-02,  7.1059e-02,\n",
       "                        8.0533e-03,  7.8902e-02,  2.9538e-03, -9.5536e-02, -6.6525e-02,\n",
       "                       -7.2543e-02, -9.2095e-02,  2.3122e-02, -6.2287e-02,  9.7087e-02,\n",
       "                       -5.5144e-02,  3.6374e-02,  4.9638e-02, -5.3479e-02, -8.6658e-02,\n",
       "                        5.8021e-02,  3.0862e-02,  3.9302e-02,  2.9331e-02, -5.4287e-03,\n",
       "                        7.8518e-02, -1.0086e-02,  8.1178e-02, -8.7617e-03, -6.4304e-02],\n",
       "                      [ 3.7788e-02,  5.6089e-02,  6.2770e-02,  7.4311e-02, -5.9083e-02,\n",
       "                       -9.9377e-02, -1.4939e-02,  1.5590e-02, -9.7545e-02, -7.6894e-02,\n",
       "                        6.5745e-02, -1.4444e-02, -6.3722e-02,  9.6849e-02, -1.0431e-07,\n",
       "                        4.4514e-03,  6.4642e-02, -2.4654e-02,  1.9522e-02, -1.6174e-02,\n",
       "                        4.5685e-02, -5.4454e-02,  8.8600e-02,  4.3506e-02, -1.2728e-02,\n",
       "                        3.9274e-02,  9.9502e-02, -2.5578e-02, -7.1254e-02,  5.6778e-02,\n",
       "                        7.4557e-02,  9.8842e-02,  9.8905e-02, -8.8813e-03, -3.3511e-02,\n",
       "                        4.8940e-02,  7.8997e-02,  7.5084e-02,  8.4941e-02, -2.1130e-02,\n",
       "                        8.4941e-02, -4.8927e-02, -3.1391e-02,  7.4892e-02,  3.5605e-02,\n",
       "                        7.1058e-02,  8.6724e-03,  4.2171e-02,  2.9162e-02,  3.8993e-02,\n",
       "                        5.1286e-02, -4.8393e-02, -3.4368e-02,  7.3344e-02,  4.1654e-02,\n",
       "                        1.7957e-02, -7.9403e-02, -4.6230e-02,  6.0001e-02,  9.5553e-02,\n",
       "                        3.8395e-02, -1.4154e-02, -9.1056e-02, -5.8697e-03,  2.5388e-02,\n",
       "                       -1.0204e-02,  9.0666e-02, -6.6528e-02,  5.1523e-02, -1.3799e-02,\n",
       "                       -8.2791e-02, -9.7647e-02, -4.9281e-02, -1.2321e-02,  9.0230e-02,\n",
       "                       -6.5240e-02,  8.4291e-02,  7.1944e-02, -1.2475e-02,  3.7563e-02,\n",
       "                        1.5627e-02, -8.0016e-02,  5.1484e-02,  5.0654e-02, -4.0679e-02,\n",
       "                        7.7997e-02, -5.8973e-02,  8.9952e-02,  6.6435e-02,  8.9470e-02,\n",
       "                        3.9458e-02, -3.1133e-02,  3.3667e-02,  4.1661e-03,  7.4472e-02,\n",
       "                       -5.8448e-02, -1.2548e-02, -8.5219e-02, -1.4149e-02, -9.5034e-02],\n",
       "                      [ 5.0568e-03,  7.5145e-02, -1.0769e-02, -8.9545e-03, -5.4289e-02,\n",
       "                        4.1025e-02, -9.8368e-02,  9.7204e-02, -9.9737e-02, -3.2909e-03,\n",
       "                        9.4029e-02,  6.9880e-02, -2.4175e-02, -6.1973e-02, -7.5397e-02,\n",
       "                        6.1826e-02, -9.2071e-02,  1.0860e-03, -4.9950e-02, -9.0327e-03,\n",
       "                        4.6888e-03,  4.3326e-02,  8.6467e-02, -5.4631e-03,  7.6808e-03,\n",
       "                       -5.1987e-02, -9.8682e-03, -8.1094e-02, -2.9614e-02, -5.2385e-03,\n",
       "                        6.9787e-02,  7.0180e-02,  4.0099e-02,  4.8756e-02, -3.5244e-02,\n",
       "                        1.0775e-03,  7.7032e-03, -4.6851e-02, -1.7488e-03, -8.1705e-03,\n",
       "                        5.8219e-02, -6.9139e-02, -9.0760e-03,  2.3731e-02, -8.6465e-03,\n",
       "                       -7.9115e-02, -5.8592e-02,  5.3537e-02, -9.8142e-03, -9.1508e-02,\n",
       "                        3.3657e-02, -1.2114e-02, -9.3338e-02, -4.6754e-02, -1.0031e-02,\n",
       "                        7.1268e-02,  4.5538e-02, -2.1177e-02, -8.5700e-02,  6.6749e-02,\n",
       "                       -1.0169e-03, -3.8731e-02,  1.5744e-02,  7.5704e-02,  5.6979e-03,\n",
       "                        6.6655e-02,  8.1582e-02, -5.4637e-02,  1.0802e-02,  6.1584e-02,\n",
       "                       -2.6939e-04,  1.2939e-02, -1.9990e-02, -6.4823e-02, -2.9683e-02,\n",
       "                       -5.8393e-02, -2.7439e-02,  1.3809e-02, -6.5434e-02,  3.9057e-02,\n",
       "                        4.6802e-02, -5.0827e-02,  1.5196e-02, -1.6033e-02, -3.9570e-02,\n",
       "                       -7.2519e-03, -5.1432e-02, -5.6723e-02, -8.6625e-02,  6.4712e-02,\n",
       "                        6.1734e-02,  3.9705e-02,  1.7131e-02,  7.2420e-02, -9.6628e-02,\n",
       "                        3.2054e-02, -6.7125e-02, -2.8398e-02,  4.8060e-02, -9.5476e-02],\n",
       "                      [ 6.9975e-02,  8.5195e-02,  5.8661e-02,  2.4201e-02, -2.5399e-02,\n",
       "                        1.9704e-02, -8.2180e-02, -5.6747e-02, -1.3827e-03,  8.2176e-02,\n",
       "                        1.6917e-02, -9.0133e-02,  5.7367e-02, -1.3327e-02,  5.3086e-02,\n",
       "                        7.2135e-02,  9.7105e-02,  1.2961e-02,  1.3990e-02, -2.8459e-02,\n",
       "                       -6.3173e-03,  8.0608e-02,  2.7868e-03, -1.7703e-04,  9.5403e-03,\n",
       "                        1.2645e-02,  5.2748e-02, -9.7726e-02, -4.8771e-02,  3.1324e-02,\n",
       "                       -1.6918e-02,  6.6040e-02, -6.1889e-02, -6.0731e-02,  3.8072e-05,\n",
       "                       -8.8594e-02, -2.9598e-02, -9.7073e-02, -2.3748e-03, -8.9247e-02,\n",
       "                       -6.5806e-02,  7.2346e-02,  1.5834e-02,  6.7299e-02, -5.5536e-02,\n",
       "                       -2.9381e-02,  4.3684e-03,  7.1397e-02,  5.1299e-02,  4.4589e-02,\n",
       "                       -9.1417e-02,  6.7682e-02, -7.3822e-02, -1.8007e-02,  4.9953e-02,\n",
       "                        1.7336e-02, -5.2147e-02,  2.2295e-02, -2.3286e-02,  3.8259e-02,\n",
       "                       -5.1454e-02, -7.6065e-02,  1.7999e-02, -6.8273e-02,  5.9642e-02,\n",
       "                        7.9697e-02, -3.3879e-02,  9.3424e-02,  4.0889e-02,  1.9237e-02,\n",
       "                       -5.7354e-02,  2.2930e-02, -3.7895e-02,  5.3297e-02,  8.1780e-02,\n",
       "                        6.8257e-02,  2.1763e-02, -4.8693e-02, -2.3489e-02,  6.1476e-02,\n",
       "                        7.9993e-02,  4.5701e-02, -7.5788e-03,  9.6841e-02,  3.1858e-03,\n",
       "                        6.1351e-02, -2.5142e-02,  8.5907e-02,  5.0316e-02,  6.8424e-02,\n",
       "                        3.9523e-02,  6.3621e-02, -1.1424e-02,  1.4072e-03,  5.0389e-02,\n",
       "                        5.0907e-03, -8.3485e-02, -5.4785e-02, -2.2367e-02,  7.1698e-02],\n",
       "                      [-7.0210e-02, -4.4820e-02, -2.8778e-02, -1.0376e-04,  7.5254e-02,\n",
       "                        1.9825e-02,  8.2203e-02, -2.8605e-02, -4.9376e-02, -3.0290e-02,\n",
       "                       -7.6878e-02,  5.8095e-02,  3.9539e-02, -3.9076e-02, -9.8240e-02,\n",
       "                        5.4035e-02, -3.0240e-02,  1.5669e-02,  5.8969e-02,  7.8447e-03,\n",
       "                       -8.1581e-03, -7.9604e-02, -4.7652e-02, -8.2708e-02,  2.6757e-02,\n",
       "                       -7.8426e-02, -7.0266e-03, -5.1791e-02, -8.5019e-02,  1.3251e-02,\n",
       "                       -6.0923e-02, -1.7375e-02,  9.7866e-02, -1.3952e-02,  5.8823e-02,\n",
       "                       -2.8713e-02,  2.3368e-02, -8.0441e-02, -6.9753e-02,  1.3443e-02,\n",
       "                       -5.7263e-02, -2.4993e-02, -1.9528e-02, -8.6995e-02,  4.7972e-02,\n",
       "                       -4.4558e-02,  5.5081e-02, -9.3170e-02, -6.7677e-02, -2.8448e-02,\n",
       "                       -4.8068e-02,  9.8727e-02,  4.0683e-02, -7.3164e-02, -7.8801e-02,\n",
       "                       -8.8955e-02,  8.3203e-03,  1.3834e-02,  6.6208e-03, -6.6042e-03,\n",
       "                       -2.2909e-02, -8.5196e-02, -9.2580e-02, -5.1662e-02, -8.6001e-02,\n",
       "                        3.0849e-02, -2.5484e-02,  8.5276e-02,  2.1406e-03,  5.8287e-02,\n",
       "                        5.2207e-02,  9.2180e-02,  8.8614e-02,  7.8127e-02, -6.5042e-02,\n",
       "                        9.7657e-02,  9.5289e-02, -8.9193e-02, -7.5520e-02,  5.1363e-03,\n",
       "                        2.3654e-02,  4.5243e-02,  2.1034e-02,  7.1382e-02, -3.5453e-02,\n",
       "                       -4.0609e-03,  2.0505e-02, -6.8304e-02,  3.3761e-02,  2.5184e-02,\n",
       "                       -9.5173e-02, -9.4185e-03,  5.7915e-02,  3.5804e-02,  2.6546e-02,\n",
       "                        3.9116e-03, -2.2518e-02,  3.5594e-02,  2.4655e-02,  2.2328e-03],\n",
       "                      [-1.7072e-02,  8.4069e-02, -8.6468e-02,  4.2322e-02, -4.4901e-02,\n",
       "                       -5.4383e-02,  1.4763e-03,  6.7732e-02,  5.2458e-02, -6.3893e-02,\n",
       "                        2.8973e-02, -1.6973e-02, -6.1703e-02,  7.1254e-02, -1.0559e-02,\n",
       "                        6.2804e-02,  2.2129e-02, -8.1381e-02, -3.1354e-02,  9.5996e-02,\n",
       "                        2.0798e-02, -5.3078e-02,  2.7681e-02,  2.4136e-02,  5.5022e-02,\n",
       "                       -7.0315e-02, -4.9052e-02,  2.3285e-02,  2.0774e-02, -9.0856e-02,\n",
       "                        2.2818e-03, -8.7891e-02, -9.1561e-02, -6.7979e-02, -8.9224e-02,\n",
       "                       -6.1802e-02, -1.3814e-02, -1.4452e-02,  4.4161e-02,  6.5529e-02,\n",
       "                        3.9477e-02, -9.4764e-02,  3.8583e-02,  2.1567e-02, -6.7490e-02,\n",
       "                       -3.6215e-03, -1.3544e-02,  1.4721e-02, -5.7439e-02,  9.5285e-02,\n",
       "                       -9.4561e-02,  9.7257e-02, -3.7876e-02,  5.7383e-02,  8.5815e-02,\n",
       "                        7.0558e-02,  3.7031e-02,  6.3982e-02,  5.1456e-02,  8.0766e-02,\n",
       "                       -1.3304e-03,  3.7506e-02,  7.3208e-04,  7.3815e-02, -3.4046e-02,\n",
       "                        9.4620e-02,  9.4273e-03, -3.9052e-03, -7.7127e-04, -6.7472e-02,\n",
       "                       -5.2380e-02,  2.0355e-04,  7.2853e-02, -3.9409e-02,  8.6969e-02,\n",
       "                       -2.2664e-02, -4.7697e-02, -7.3366e-02, -6.2103e-03,  7.5588e-03,\n",
       "                        6.0012e-02, -4.6344e-02,  5.9230e-02,  6.2186e-02,  2.0421e-02,\n",
       "                        3.9521e-02, -8.6657e-02,  7.4828e-02,  1.1784e-02, -9.4827e-02,\n",
       "                       -9.8249e-02, -8.7157e-02,  3.2007e-02, -4.6245e-02, -9.9963e-02,\n",
       "                       -8.4891e-02,  4.2076e-03,  9.2683e-02, -9.9623e-02,  8.0965e-02]])),\n",
       "             ('layer3.0.bias',\n",
       "              tensor([ 0.0920, -0.0022,  0.0223, -0.0661,  0.0231, -0.0612, -0.0273, -0.0044,\n",
       "                      -0.0458,  0.0734]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41f456e6-0da5-4895-b69b-4662c958b9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42658718-9329-4639-83cf-96082646fdee",
   "metadata": {},
   "source": [
    "<h2>3. Pick a loss function and optimizer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2615fb84-4220-4797-9e4e-48e45549a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e70d7-978b-42af-b433-9cee68d5de8b",
   "metadata": {},
   "source": [
    "<h2>4. Building training loop</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3188696a-d605-4f1e-a109-abeb7a482320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 12\u001b[0m out \u001b[38;5;241m=\u001b[39m model(img)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out,label)\n\u001b[0;32m     14\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m---> 17\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m     18\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(a)\n\u001b[0;32m     19\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(b)\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "num_epochs = 6\n",
    "for epoch in range(num_epochs):\n",
    "    print('*' * 10)\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    running_loss = 0.0\n",
    "    running_accu = 0.0\n",
    "    for i,data in enumerate(train_loader,1):\n",
    "        img,label = data\n",
    "        img = img.view(img.size(0),-1)\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        out = model(img)\n",
    "        loss = criterion(out,label)\n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(out,1) #64,10\n",
    "        running_accu += (pred == label).float().mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 300 == 0:\n",
    "            print(f'Epoch {epoch + 1} / {num_epochs}, Loss: {running_loss / i:.6f}, Accu: {running_accu / i:.6f}')\n",
    "        print(f'Finish: {epoch + 1} Epoch, Loss {running_loss / i:.6f}, Accu: {running_accu / i:.6f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e250af-5bf7-454a-925f-2f884aaf1f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
