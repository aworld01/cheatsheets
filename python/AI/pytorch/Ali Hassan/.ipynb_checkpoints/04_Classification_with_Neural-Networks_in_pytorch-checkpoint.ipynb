{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f202c68-6a7e-45ce-8fbd-26c925f6c24f",
   "metadata": {},
   "source": [
    "<h1>40:00/1:23:15</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f45794-8bdf-4418-8b9a-d2c73e78402d",
   "metadata": {},
   "source": [
    "<h1>Workflow</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd49570-f7b3-4e5d-ada0-6c0d5b448056",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    0. Import important libraries<br>\n",
    "    1. Get dataset ready (turn into tensor and batches)<br>\n",
    "    2. Build a NeuralNetwork model for classification<br>\n",
    "    3. Pick a loss function and optimizer<br>\n",
    "    4. Build a training loop<br>\n",
    "    5. Evaluate your model<br>\n",
    "    6. Improve your model<br>\n",
    "    7. Save the model\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f0b7e-5270-427e-b150-0574d7344094",
   "metadata": {},
   "source": [
    "<h2>0. Import important libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b63d2a9c-904b-4020-a408-065b5198f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dec84e-5a64-4a1f-9d70-a7a41dfb934e",
   "metadata": {},
   "source": [
    "<h2>1. Get dataset ready (turn into tensor and batches)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd435765-2141-4d17-8b5c-7038c59efd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.FashionMNIST(root='/dataset',train=True,transform=transforms.ToTensor(),download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='/dataset',train=False,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1a7dd70-4be5-4566-b3c7-12a801fe60ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /dataset\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset #to check dataset items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e51e47f-8209-4cbc-9914-b6f6d5edae11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: /dataset\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset #to check dataset items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06bd1589-be7b-4194-8ef2-5b35f7cf6c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83527e-3bb2-4e10-a207-fdd68be4c535",
   "metadata": {},
   "source": [
    "<h3>1.2 Conveting data into batches</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719061ae-4852-4e2f-be74-32be1a952b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d20313e0-5275-4d52-a9d2-9361bee598fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader)) #60000 / 64 = 938\n",
    "print(len(test_loader)) #10000 / 64 = 157"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cb0069-010f-4e05-9ac5-61ea13fc1918",
   "metadata": {},
   "source": [
    "<h2>2. Build a Neural Network model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53db2484-bc27-4fdf-80e7-4f578e514523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,in_dim,n_hidden_1,n_hidden_2,out_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_dim,n_hidden_1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(n_hidden_1,n_hidden_2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(n_hidden_2,out_dim),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        a = self.layer1(x)\n",
    "        b = self.layer2(a)\n",
    "        c = self.layer3(b)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "338a444f-a89f-4add-831f-88443d01919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = 28*28 #784\n",
    "n_hidden_1 = 300\n",
    "n_hidden_2 = 100\n",
    "out_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c265ce-10ca-4130-8b92-348cd6df192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(in_dim,n_hidden_1,n_hidden_2,out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4edc4607-5a10-4b22-830a-92f7171239ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=300, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=10, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "317f5612-d6b4-4f70-af5e-882765ea81be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.0.weight',\n",
       "              tensor([[-2.0947e-05, -1.0681e-02,  1.4346e-02,  ...,  2.8914e-02,\n",
       "                        2.4745e-02,  3.0206e-02],\n",
       "                      [ 2.3531e-02,  2.2868e-02,  3.5545e-02,  ...,  1.4363e-02,\n",
       "                        1.2954e-02,  1.6631e-02],\n",
       "                      [ 2.5726e-02, -3.2099e-02, -1.3621e-02,  ...,  3.1739e-02,\n",
       "                        1.6105e-02, -2.6410e-02],\n",
       "                      ...,\n",
       "                      [ 5.2864e-03,  2.8340e-02, -7.5645e-03,  ...,  2.6541e-02,\n",
       "                       -3.1296e-02,  1.1929e-02],\n",
       "                      [-1.0672e-02, -1.0607e-02,  2.1536e-02,  ..., -1.5379e-02,\n",
       "                       -2.0171e-02,  3.4121e-02],\n",
       "                      [ 3.4703e-03,  8.0338e-03, -4.4267e-03,  ..., -2.2835e-02,\n",
       "                       -1.0904e-02,  1.8417e-02]])),\n",
       "             ('layer1.0.bias',\n",
       "              tensor([ 7.9297e-03, -3.1590e-02, -1.6715e-02,  2.9978e-02, -2.5238e-02,\n",
       "                      -1.9755e-02, -6.3326e-03,  2.4279e-02, -2.1490e-02,  5.1158e-04,\n",
       "                      -1.7850e-02,  6.3033e-03, -4.0592e-03, -3.5199e-02, -1.4805e-02,\n",
       "                      -3.2879e-02,  6.9798e-03, -2.5119e-02,  1.9761e-02,  3.5115e-02,\n",
       "                      -6.8162e-03,  2.5851e-02, -1.4330e-02, -3.5524e-02,  3.2492e-02,\n",
       "                      -6.5184e-03,  3.1073e-02, -9.4272e-03, -1.9522e-02,  2.1796e-02,\n",
       "                      -2.5910e-02, -2.2483e-02, -2.9880e-02,  3.4204e-02,  3.2996e-02,\n",
       "                      -3.3884e-02,  1.3586e-02, -3.5041e-02, -2.7684e-02,  9.2256e-03,\n",
       "                      -1.8654e-02,  1.1501e-02,  1.7998e-02,  2.0996e-02, -2.0231e-02,\n",
       "                       1.1017e-02, -3.2145e-02, -2.4981e-02,  2.6407e-02, -3.3216e-02,\n",
       "                      -2.9283e-02,  1.6998e-02,  5.0634e-03, -3.2304e-02, -1.7996e-03,\n",
       "                       6.8494e-03,  1.8949e-02, -2.2074e-02,  2.9965e-02,  1.5696e-02,\n",
       "                       7.4530e-03, -3.3397e-03,  6.3860e-03,  2.7155e-02, -1.9181e-02,\n",
       "                       1.8939e-02,  9.4120e-03, -1.0584e-03,  1.3688e-02, -6.4392e-03,\n",
       "                      -2.3810e-02, -2.1357e-03,  1.5001e-02, -3.1080e-02, -9.0989e-03,\n",
       "                       2.8384e-02, -6.4407e-04,  8.2604e-03, -3.1793e-02, -3.0592e-02,\n",
       "                      -2.0402e-02,  1.3950e-02,  2.8126e-02, -1.7431e-02, -1.8495e-02,\n",
       "                       7.0640e-03,  1.0800e-02,  1.5153e-02,  2.8439e-02, -2.9915e-02,\n",
       "                       1.2480e-02,  1.7758e-02,  2.4309e-02,  1.3624e-02, -3.1433e-02,\n",
       "                      -3.4137e-02, -5.0407e-03,  2.7142e-02,  1.5956e-02, -2.5972e-02,\n",
       "                      -6.1606e-03, -2.0690e-02, -6.7096e-05,  7.9811e-03, -4.5885e-03,\n",
       "                      -2.2274e-02,  3.1311e-02, -1.4226e-04, -3.0650e-02,  1.9418e-02,\n",
       "                      -1.6651e-02, -2.5933e-03, -1.7135e-03, -3.6061e-03, -1.3856e-02,\n",
       "                      -1.9846e-02, -2.3791e-02,  2.4231e-02, -2.2947e-02,  2.7935e-02,\n",
       "                       3.2253e-02, -2.0098e-04,  1.6258e-02,  3.1848e-02, -3.1741e-02,\n",
       "                      -3.6362e-03, -1.8401e-02, -1.6998e-03, -4.9700e-03, -4.8374e-03,\n",
       "                       2.4685e-02, -1.5363e-02,  1.1725e-02, -3.4934e-02, -4.0386e-03,\n",
       "                       5.2647e-03,  1.7458e-02, -3.4667e-02, -2.3098e-02, -7.0485e-04,\n",
       "                       2.5287e-02, -1.0126e-02, -1.5007e-02, -5.9791e-03,  1.7172e-02,\n",
       "                      -2.0765e-02,  7.2009e-03, -2.7048e-02,  4.0814e-03, -1.2987e-02,\n",
       "                      -2.8714e-02,  5.4035e-03,  1.4131e-02, -3.5276e-02,  2.3418e-02,\n",
       "                       1.3424e-02, -1.8541e-02,  6.4749e-03,  2.3940e-03, -4.6480e-03,\n",
       "                      -2.1652e-02,  3.4830e-02,  2.5930e-02,  1.0840e-02, -8.5924e-05,\n",
       "                      -3.3460e-02,  1.4031e-02, -2.9338e-02, -2.1956e-02, -1.6150e-02,\n",
       "                       1.3059e-02, -6.6010e-04,  1.8917e-02, -2.2460e-02, -2.5150e-02,\n",
       "                       2.3394e-03,  2.2365e-02, -1.1262e-02,  2.2850e-02,  1.7248e-02,\n",
       "                      -3.9265e-03, -7.3641e-03,  3.1984e-02, -1.4063e-02, -1.4231e-02,\n",
       "                       2.0209e-02, -2.8134e-02,  2.9214e-02, -1.5679e-02,  2.2897e-04,\n",
       "                       2.8083e-02, -2.0092e-02, -1.9002e-02,  3.3266e-02, -5.5585e-03,\n",
       "                      -3.0494e-02,  7.5986e-03,  9.1763e-03, -3.2912e-02,  3.2262e-02,\n",
       "                      -2.5591e-02,  2.6618e-02, -3.5448e-02, -1.1219e-02,  9.6481e-04,\n",
       "                      -2.9477e-03,  4.0575e-03, -1.8994e-02,  1.5221e-02, -3.2135e-02,\n",
       "                      -3.3177e-02, -4.4381e-03,  9.5502e-03,  5.8865e-03, -1.6647e-02,\n",
       "                       2.1210e-02, -2.3846e-02,  1.9836e-02,  1.0182e-02, -3.4155e-02,\n",
       "                      -3.2336e-03,  1.5026e-02,  2.5843e-02,  1.0839e-02,  2.7679e-02,\n",
       "                       2.4081e-02, -1.7753e-02,  2.5065e-02, -2.8953e-02, -1.2371e-02,\n",
       "                       2.4371e-03,  4.5701e-03,  2.2544e-02,  1.5480e-03, -4.8892e-03,\n",
       "                      -1.5505e-02, -2.1138e-02, -2.6472e-02,  1.5549e-02, -3.1873e-02,\n",
       "                      -1.6155e-02,  2.2152e-02, -2.2328e-02,  2.8688e-02,  1.5285e-02,\n",
       "                      -1.4162e-02,  1.7431e-03, -1.3433e-02, -2.6389e-02, -2.4732e-02,\n",
       "                       3.4144e-02,  1.7072e-02, -2.3348e-02,  3.9935e-03, -2.7074e-02,\n",
       "                      -1.4698e-02, -2.5501e-02, -4.2252e-05,  3.1799e-02, -2.8423e-02,\n",
       "                      -1.1172e-02,  5.1077e-03,  1.9964e-02, -3.2670e-02, -1.2742e-02,\n",
       "                      -3.5137e-02,  1.2243e-02, -8.3543e-03, -2.3441e-02,  2.7568e-02,\n",
       "                       2.0928e-02,  3.2105e-02,  5.4926e-03, -1.9728e-02,  1.7184e-02,\n",
       "                      -3.1066e-02,  1.3800e-02, -2.9467e-02, -3.2963e-02,  2.5244e-02,\n",
       "                      -3.5453e-02,  3.3301e-02,  1.3679e-02, -3.1177e-02, -4.3977e-03,\n",
       "                       2.3679e-02,  1.0027e-02, -2.6325e-02,  2.4710e-02,  3.0315e-02,\n",
       "                      -2.4108e-02, -1.6232e-02, -1.1705e-02, -1.4075e-02,  2.9489e-02,\n",
       "                       9.9418e-03,  3.5091e-02,  1.0077e-02,  1.1196e-02, -2.2325e-02])),\n",
       "             ('layer2.0.weight',\n",
       "              tensor([[-0.0450, -0.0347,  0.0504,  ...,  0.0153, -0.0208,  0.0052],\n",
       "                      [ 0.0532, -0.0281, -0.0556,  ...,  0.0538,  0.0143, -0.0133],\n",
       "                      [-0.0119, -0.0405, -0.0477,  ...,  0.0567, -0.0329,  0.0134],\n",
       "                      ...,\n",
       "                      [-0.0573,  0.0077,  0.0055,  ...,  0.0138, -0.0057,  0.0016],\n",
       "                      [ 0.0207, -0.0511,  0.0307,  ..., -0.0226, -0.0508, -0.0462],\n",
       "                      [-0.0030, -0.0222,  0.0545,  ...,  0.0418, -0.0138,  0.0129]])),\n",
       "             ('layer2.0.bias',\n",
       "              tensor([-4.7137e-02, -4.8752e-02, -5.0488e-02, -1.7326e-02, -2.1230e-02,\n",
       "                       1.1444e-02,  3.1642e-02, -2.2047e-02,  1.9232e-02, -9.6497e-03,\n",
       "                       6.3230e-03,  1.2516e-02,  2.0924e-02,  5.6844e-02, -3.2550e-03,\n",
       "                       5.2530e-02, -5.5705e-02,  5.4537e-02, -5.1433e-02,  3.1489e-02,\n",
       "                       2.5912e-02,  8.1912e-03,  4.7824e-02, -9.9054e-04,  1.4941e-02,\n",
       "                      -5.4717e-02,  2.7258e-02,  2.6490e-02, -2.9548e-02,  7.8912e-03,\n",
       "                      -4.5750e-02, -3.4786e-02, -4.7130e-03,  3.0783e-02,  3.4334e-02,\n",
       "                       3.2386e-02, -2.9786e-03, -1.4639e-02, -1.1167e-02, -2.3838e-02,\n",
       "                      -3.4506e-02,  3.5247e-02, -9.5398e-03,  2.4841e-02,  2.6355e-02,\n",
       "                       5.0803e-03,  4.6021e-02,  3.1440e-02, -3.8050e-02, -2.6529e-02,\n",
       "                       6.9664e-04,  5.6121e-02,  5.1592e-02,  4.2615e-02,  9.1980e-03,\n",
       "                       5.3178e-02, -2.1791e-02,  2.4141e-02, -3.4376e-02,  5.7819e-03,\n",
       "                       2.4121e-02, -9.1687e-04,  2.6808e-02,  3.6573e-02,  5.3421e-06,\n",
       "                      -4.2772e-03, -6.4206e-03, -1.0736e-02, -4.2183e-02, -1.6212e-02,\n",
       "                      -1.4740e-02,  5.0316e-03, -2.2596e-02, -4.9755e-02, -2.2399e-02,\n",
       "                       7.6213e-03, -1.0720e-03, -1.7558e-02,  5.7496e-02,  2.2308e-03,\n",
       "                      -2.0298e-02, -5.4926e-02, -2.4456e-02,  1.1351e-02, -3.2005e-02,\n",
       "                       5.1196e-02,  4.5854e-02,  4.2182e-02, -5.5335e-02,  3.5809e-02,\n",
       "                       3.9345e-02, -3.4363e-02, -1.8145e-02, -2.8181e-02,  3.9301e-04,\n",
       "                      -4.9804e-02, -4.4612e-02,  5.7410e-02, -3.6872e-02,  4.3504e-02])),\n",
       "             ('layer3.0.weight',\n",
       "              tensor([[-3.1126e-02,  6.5700e-02,  3.2286e-02,  1.5582e-02,  5.9511e-02,\n",
       "                       -7.0439e-02,  7.0754e-03,  7.0143e-02,  3.8089e-02, -7.6078e-02,\n",
       "                       -1.1395e-02,  4.9486e-03,  2.3602e-02,  9.8478e-02,  4.2290e-02,\n",
       "                        5.8590e-02,  1.3904e-02,  3.4219e-02,  3.7031e-02, -9.2136e-03,\n",
       "                       -1.3656e-02,  8.7495e-02,  2.0731e-02, -9.0103e-02, -3.1001e-02,\n",
       "                       -9.9136e-02,  9.3350e-02,  3.8131e-02,  8.3714e-02, -3.2908e-02,\n",
       "                        8.5029e-02, -7.2111e-02,  8.2893e-02,  3.1990e-02, -9.2026e-02,\n",
       "                        5.7123e-02,  5.4775e-02, -9.1765e-02,  3.5512e-02,  4.7463e-02,\n",
       "                        2.2841e-02,  8.0669e-02, -7.8598e-02,  1.0415e-02,  1.4614e-02,\n",
       "                       -9.4249e-02,  9.1351e-02, -1.2296e-02,  8.3685e-02, -9.2549e-02,\n",
       "                       -1.4383e-02,  8.1592e-02,  1.1012e-02, -5.6229e-02,  3.6694e-02,\n",
       "                        9.3763e-02, -3.2570e-02, -1.4664e-02,  9.2674e-02, -1.1371e-02,\n",
       "                       -3.4983e-02,  8.7126e-02, -9.9131e-02, -9.9899e-02,  9.5772e-02,\n",
       "                       -1.0708e-02,  7.9518e-02,  9.8363e-02,  9.0641e-03,  1.0302e-02,\n",
       "                       -6.6516e-02,  9.4850e-03, -7.7702e-02,  9.2120e-02, -9.4064e-02,\n",
       "                        9.4526e-02, -8.1377e-02,  8.2129e-02,  1.1205e-02,  7.3711e-03,\n",
       "                       -6.6049e-02,  1.6867e-02, -5.4272e-03,  8.4078e-02,  1.5324e-03,\n",
       "                        6.1370e-03,  7.2260e-02, -8.7880e-02,  4.9981e-02, -7.4080e-02,\n",
       "                        3.1172e-02,  2.6846e-02,  4.7012e-02, -6.9199e-02, -3.8582e-02,\n",
       "                       -8.0982e-02, -4.7800e-02, -1.5540e-02, -1.9124e-02,  8.5949e-02],\n",
       "                      [-5.8378e-02,  7.7281e-02, -4.9019e-02,  4.5245e-02, -2.6268e-02,\n",
       "                       -9.6645e-02,  3.6655e-02,  5.9631e-02, -3.8051e-03, -5.8681e-02,\n",
       "                        4.7717e-02,  8.8589e-02, -4.1656e-02,  7.7155e-02,  2.9166e-02,\n",
       "                       -7.3219e-02, -6.8475e-02,  1.8520e-02,  1.5781e-02,  7.9391e-03,\n",
       "                       -8.7605e-03,  5.4151e-02,  1.2719e-02, -8.7047e-02, -8.8931e-02,\n",
       "                       -2.7695e-02, -8.3304e-02,  6.8247e-02, -9.9519e-02,  4.3979e-02,\n",
       "                       -3.0122e-02,  3.7714e-02,  6.5373e-02,  3.9190e-02,  4.5848e-02,\n",
       "                       -9.0941e-02,  6.6352e-02, -7.3489e-02,  4.6727e-02, -1.1337e-02,\n",
       "                        9.4068e-02, -6.4847e-02,  9.8086e-02, -6.4920e-02,  3.5834e-02,\n",
       "                       -4.3772e-02, -5.6570e-02, -5.7564e-02, -3.4574e-02, -5.1606e-02,\n",
       "                        7.7519e-02, -9.5884e-02, -3.7471e-02,  6.3410e-02,  8.6811e-02,\n",
       "                        4.1259e-02, -8.7958e-02, -7.7886e-02,  9.1338e-02, -1.5476e-02,\n",
       "                       -4.5795e-02,  6.0971e-03, -7.1625e-02, -1.1015e-02,  1.7874e-02,\n",
       "                        7.2219e-02,  6.9077e-02,  8.7212e-03, -4.9946e-02, -9.8513e-02,\n",
       "                       -6.1465e-02,  1.4180e-02, -1.1434e-02,  8.2900e-02, -7.9983e-03,\n",
       "                        5.5413e-02, -8.8123e-02, -9.3625e-02,  4.6356e-02,  5.6948e-02,\n",
       "                       -7.4400e-02,  7.7646e-02,  9.5361e-02,  6.9836e-02, -9.8124e-02,\n",
       "                        1.9749e-02, -1.7873e-02,  6.3056e-02, -3.6322e-05, -4.1380e-02,\n",
       "                       -4.0184e-02,  1.6900e-02,  7.3701e-02,  1.4835e-02,  1.9758e-02,\n",
       "                       -4.0903e-02, -1.0197e-02,  6.7601e-02, -3.8185e-02, -9.4702e-02],\n",
       "                      [ 3.3702e-02, -4.9794e-02, -3.9792e-03,  6.2036e-02,  5.6234e-02,\n",
       "                       -5.3862e-02,  2.2535e-02,  3.3466e-03, -2.0829e-02, -2.6768e-02,\n",
       "                        7.7107e-02,  4.7085e-02, -5.9250e-02, -4.3834e-02,  4.2248e-02,\n",
       "                       -1.2168e-02, -3.8574e-02,  1.9372e-02,  1.6675e-02, -9.2604e-02,\n",
       "                        5.6821e-02, -4.7753e-02, -8.1867e-03, -6.0797e-06,  1.3748e-02,\n",
       "                       -2.8751e-02,  4.0823e-02,  7.1858e-02, -5.4044e-02,  6.1781e-02,\n",
       "                        1.9528e-02,  8.2248e-02,  8.1868e-02, -9.6757e-02,  8.9871e-02,\n",
       "                       -8.7272e-02, -6.1983e-02,  4.2417e-02,  3.5473e-02,  2.9364e-03,\n",
       "                       -2.9147e-02,  9.3068e-02,  4.8433e-02,  3.4695e-02,  4.6879e-02,\n",
       "                       -6.4385e-02,  6.2021e-02,  5.3344e-02,  7.9609e-02, -3.6941e-02,\n",
       "                        7.7321e-02, -6.5720e-02,  2.9795e-02,  9.8312e-02,  8.4807e-02,\n",
       "                       -8.9074e-02,  6.7808e-02,  2.0751e-02,  5.8374e-03,  2.9014e-02,\n",
       "                        6.0676e-02,  7.3604e-02, -5.1692e-02, -7.2864e-02,  8.8443e-02,\n",
       "                        9.0548e-02,  3.1816e-02,  7.5321e-02,  4.5508e-02,  4.9312e-02,\n",
       "                       -1.1367e-02, -8.5255e-02, -8.9640e-02,  5.0266e-03, -5.8601e-02,\n",
       "                       -8.3960e-02, -1.6484e-02,  8.7430e-02,  9.5893e-02, -3.1303e-02,\n",
       "                        6.6804e-02,  5.1219e-02,  8.0881e-02,  3.3722e-02,  3.3225e-02,\n",
       "                       -3.7681e-02,  5.2687e-02,  5.1215e-04,  9.0775e-02,  8.2468e-03,\n",
       "                        5.8511e-02, -1.8162e-02, -6.0784e-02,  9.1202e-02, -5.1954e-02,\n",
       "                        1.2950e-02, -8.8410e-02, -9.2620e-03, -9.4970e-02,  4.8101e-02],\n",
       "                      [-5.7562e-04, -3.9225e-02,  7.1342e-02, -4.9464e-02,  8.9777e-02,\n",
       "                        8.6854e-02,  1.5245e-02,  5.1044e-02,  1.3831e-02, -6.1929e-02,\n",
       "                       -9.5946e-03,  4.9956e-02,  1.3385e-02, -5.1275e-02, -9.5046e-02,\n",
       "                        1.8598e-02, -4.2454e-02, -1.0086e-02, -5.3870e-03, -9.3990e-02,\n",
       "                       -5.8124e-02,  9.7152e-02,  8.6568e-05, -6.5145e-02, -6.2412e-02,\n",
       "                       -4.5692e-02, -7.6804e-03,  3.9208e-02, -2.9344e-02, -6.0622e-04,\n",
       "                       -4.2643e-02, -1.6122e-02,  3.6334e-02,  5.9958e-02,  2.4293e-02,\n",
       "                        8.9661e-02,  8.1833e-02,  7.6459e-02,  1.2611e-02, -4.3011e-02,\n",
       "                        5.7116e-02, -2.9736e-03,  5.4000e-03,  2.6626e-02, -3.3057e-02,\n",
       "                        4.9196e-02,  7.9625e-02,  5.9919e-02,  3.2598e-02,  4.4890e-02,\n",
       "                       -2.2721e-02,  6.9920e-02,  5.0082e-02, -9.2674e-02, -1.7756e-02,\n",
       "                       -3.7027e-02,  9.9912e-02,  1.0709e-02,  2.9567e-02, -9.6758e-02,\n",
       "                        9.3465e-02,  4.5818e-02,  1.4911e-02,  3.8116e-02, -3.7474e-02,\n",
       "                        1.2304e-02,  3.1844e-03,  8.7770e-02,  1.5853e-02,  1.1660e-02,\n",
       "                        6.8593e-02, -3.3358e-02,  3.3575e-02,  3.7671e-02,  5.1005e-02,\n",
       "                       -4.9339e-02, -8.5627e-02,  5.0243e-03, -3.9184e-02,  4.5197e-02,\n",
       "                        2.6822e-03, -4.5087e-02,  3.0265e-02,  4.5520e-02,  1.7735e-02,\n",
       "                       -7.3096e-02,  9.0177e-02, -7.3204e-02, -6.0898e-02,  6.0866e-02,\n",
       "                        3.2064e-02, -9.7642e-02,  2.0169e-02, -3.4792e-02,  3.9752e-02,\n",
       "                        6.4631e-02,  2.0878e-02,  2.8088e-02,  8.3812e-03,  9.1359e-02],\n",
       "                      [-8.6426e-02,  4.6605e-02, -4.0597e-02, -3.1389e-02, -5.7676e-02,\n",
       "                        6.2764e-02, -3.3091e-02,  3.2010e-02, -8.8264e-02, -4.4396e-02,\n",
       "                        3.8002e-02,  8.8743e-02, -6.2821e-03,  3.3272e-02,  5.6626e-02,\n",
       "                        7.3901e-02,  9.0186e-02, -7.3316e-04,  3.7639e-02, -6.6175e-02,\n",
       "                        4.7306e-03,  8.2020e-02,  1.1448e-02, -7.9822e-02,  5.9086e-02,\n",
       "                        7.6608e-02, -2.2985e-02, -3.6837e-02,  2.8416e-02, -7.5361e-02,\n",
       "                       -4.6795e-02, -1.4499e-03, -8.1023e-02, -5.2223e-02,  2.4026e-02,\n",
       "                       -4.1618e-03, -3.5087e-03,  1.0280e-02,  7.1402e-02, -5.1431e-03,\n",
       "                        8.3656e-02,  4.7231e-02,  9.2747e-02, -1.9078e-02, -5.0928e-02,\n",
       "                        7.0288e-02, -3.5556e-02,  4.0142e-02,  3.5526e-04, -7.7702e-02,\n",
       "                        8.2796e-02,  5.1852e-02, -7.8481e-02, -7.0215e-02,  3.8150e-02,\n",
       "                        2.4598e-02, -3.9425e-02,  7.7011e-02,  2.7350e-02, -4.9369e-03,\n",
       "                        5.4188e-02,  6.9999e-02, -7.5943e-02,  6.1858e-02,  6.9290e-02,\n",
       "                       -5.3695e-02, -5.3015e-02,  2.7816e-02, -6.9887e-02,  2.9005e-02,\n",
       "                       -6.8720e-02,  6.7864e-02, -9.2008e-02, -9.4854e-02, -3.7771e-02,\n",
       "                       -3.9672e-02, -4.4160e-02,  4.1134e-02, -5.4391e-02, -3.8762e-03,\n",
       "                        9.5343e-02,  9.1491e-02, -7.7022e-02,  6.4233e-03,  1.4910e-02,\n",
       "                        4.3735e-02, -2.5701e-02, -2.2402e-02,  1.9735e-02,  3.4611e-02,\n",
       "                       -5.1708e-02,  4.9749e-02,  8.4887e-02,  5.9107e-02,  1.8770e-02,\n",
       "                       -8.9262e-02,  8.8344e-02,  3.2066e-02,  9.2340e-02,  3.7427e-02],\n",
       "                      [-5.5459e-02,  1.3871e-02,  4.1464e-02,  4.0467e-02,  2.2093e-02,\n",
       "                       -5.6147e-02,  2.1344e-02, -4.3213e-02,  9.0851e-03, -3.7776e-02,\n",
       "                        2.3621e-02,  4.7912e-02,  9.0666e-02, -6.7867e-02,  9.2767e-02,\n",
       "                       -7.0082e-02,  1.7601e-02,  3.8671e-02, -1.0163e-02, -1.1309e-02,\n",
       "                        6.6250e-02,  2.0899e-02,  4.3948e-02, -8.2217e-03,  1.5950e-02,\n",
       "                       -1.3820e-02, -3.2771e-02, -4.7189e-02,  5.5375e-03, -9.4568e-02,\n",
       "                        8.0305e-02,  7.9208e-02,  3.8346e-02, -6.8680e-02, -6.2422e-02,\n",
       "                        9.5707e-02,  3.7825e-02,  3.1141e-02, -2.6113e-02,  8.0005e-02,\n",
       "                        5.2391e-02, -1.7816e-02,  3.1807e-02,  4.9466e-02,  5.9647e-02,\n",
       "                        2.7385e-02, -3.7189e-02,  1.6225e-02, -3.5013e-02,  2.6446e-02,\n",
       "                       -3.1981e-02, -8.5602e-02, -3.6458e-02, -6.0842e-02, -1.6527e-02,\n",
       "                        4.3260e-02,  1.1205e-03,  6.0225e-02,  7.5593e-02,  2.1298e-02,\n",
       "                        7.0223e-02, -4.3462e-02, -4.1242e-02, -2.9531e-02,  6.5895e-04,\n",
       "                        7.2308e-02,  1.8063e-02, -7.5509e-02,  7.1539e-02, -5.3368e-02,\n",
       "                       -8.3952e-02, -3.5143e-02, -7.5626e-02, -2.2796e-02,  5.8804e-02,\n",
       "                        2.2290e-02,  4.2168e-02,  7.3541e-02,  6.9261e-02, -7.0289e-02,\n",
       "                        9.9207e-02,  8.6547e-03,  5.3148e-02,  3.8427e-02, -5.3854e-02,\n",
       "                       -4.7136e-02,  5.8479e-02, -8.1039e-02, -4.3860e-02,  3.3427e-02,\n",
       "                       -8.6902e-02,  7.4099e-02,  5.0943e-02,  5.0862e-02, -9.6547e-02,\n",
       "                        9.0445e-02,  8.5703e-02,  1.7063e-02, -1.5122e-03,  6.0272e-02],\n",
       "                      [ 6.6547e-02,  9.0994e-02, -4.6299e-02, -4.4112e-02,  4.5015e-02,\n",
       "                       -5.9800e-02,  3.9794e-02, -8.0411e-02, -9.3560e-02,  6.7213e-02,\n",
       "                       -6.7351e-02,  8.5211e-02,  6.2698e-02,  5.3622e-02,  8.4738e-02,\n",
       "                       -8.5796e-02,  2.8779e-02, -5.6008e-02,  9.2282e-02,  3.8262e-02,\n",
       "                        6.7757e-02, -5.8797e-02, -9.4546e-02,  1.1568e-02, -3.2565e-02,\n",
       "                        3.3175e-02,  1.6018e-02,  8.2502e-02, -9.6214e-02, -4.5360e-02,\n",
       "                        2.7872e-02,  2.3586e-02, -6.1099e-02, -9.8240e-03, -1.0379e-02,\n",
       "                        7.1108e-02,  4.4263e-02,  8.9118e-02,  6.8173e-02,  8.2978e-02,\n",
       "                        4.3554e-02,  4.6469e-02, -6.6877e-03, -1.0379e-02,  4.0707e-02,\n",
       "                       -6.4351e-02,  2.1667e-03, -3.4337e-02,  9.8670e-02,  9.6705e-02,\n",
       "                        1.6298e-02,  3.7511e-02,  3.0825e-02, -8.6236e-02, -5.9984e-02,\n",
       "                       -3.1973e-02, -1.8321e-02, -5.0681e-02, -5.2570e-02, -4.4236e-02,\n",
       "                       -3.0335e-02, -2.4325e-02, -9.3480e-02,  1.2017e-02, -8.7113e-02,\n",
       "                        3.0116e-02, -6.8679e-02, -7.4425e-02,  1.3913e-02, -2.7312e-02,\n",
       "                       -5.3136e-02,  5.6288e-02, -9.4323e-02,  1.3962e-02,  9.5727e-02,\n",
       "                       -7.0386e-02,  2.1959e-02,  6.5055e-02, -8.0247e-02, -2.9777e-02,\n",
       "                       -7.2269e-02,  6.4708e-02,  6.4313e-02,  3.9825e-02, -8.2863e-02,\n",
       "                        7.7119e-02,  2.4348e-02,  4.5409e-02, -1.9293e-02, -2.0774e-02,\n",
       "                       -6.5664e-02,  8.5626e-02, -2.0902e-02,  1.5898e-02,  7.9250e-03,\n",
       "                        6.5405e-02,  9.9802e-02,  4.2343e-02,  7.4348e-02, -1.7815e-02],\n",
       "                      [ 1.2813e-02,  2.0547e-02, -8.9830e-03, -2.1502e-02, -7.9339e-02,\n",
       "                        1.8175e-03, -9.7595e-02, -5.7840e-03,  8.5362e-02,  4.8215e-02,\n",
       "                        6.5201e-02, -9.7778e-03, -6.1210e-03,  1.7985e-02, -1.3753e-02,\n",
       "                        8.5077e-02, -2.1916e-02,  3.1313e-02,  2.7973e-03,  7.6377e-02,\n",
       "                        7.8761e-02, -7.3739e-02, -1.6008e-02,  3.4885e-02,  3.3048e-02,\n",
       "                       -3.1484e-02, -6.2309e-02, -4.5805e-03, -1.2791e-02, -4.9166e-02,\n",
       "                       -6.3421e-02,  4.1403e-02,  1.3728e-02, -8.8211e-02,  5.8844e-02,\n",
       "                       -6.0445e-02, -6.0691e-03, -9.7740e-02,  9.2660e-02, -3.1019e-02,\n",
       "                       -8.3596e-02, -9.5811e-02, -4.9230e-02, -4.8142e-02,  2.4618e-02,\n",
       "                       -1.6716e-02, -7.8628e-03,  2.4749e-02,  1.3513e-02, -3.3007e-02,\n",
       "                       -2.5172e-02,  5.2562e-02,  8.3051e-02,  4.7671e-02,  1.1459e-02,\n",
       "                       -9.0047e-02,  8.2591e-02,  4.0918e-02,  2.6876e-02,  5.2053e-02,\n",
       "                        1.9607e-02,  1.3808e-02,  4.7185e-02, -6.4694e-02, -8.1418e-02,\n",
       "                        9.7940e-03, -3.2539e-02,  8.0598e-02, -8.6324e-02,  5.3108e-02,\n",
       "                        3.3678e-02, -1.8810e-02,  9.1483e-02,  4.6125e-02,  3.1760e-02,\n",
       "                       -3.2174e-02, -4.3217e-02,  7.8452e-02,  2.2895e-02, -2.9567e-02,\n",
       "                       -4.7068e-02,  2.0193e-02, -5.5177e-02,  9.1094e-02,  7.6868e-02,\n",
       "                        2.6130e-02,  9.8821e-02, -3.5727e-02, -4.1190e-02,  1.0824e-02,\n",
       "                       -4.1222e-02,  4.7434e-02,  8.9916e-02, -3.8673e-02,  7.5036e-02,\n",
       "                       -1.0539e-03,  4.9088e-02, -5.1752e-03,  8.6896e-02, -9.1447e-02],\n",
       "                      [-9.8946e-03,  5.1101e-02, -6.0681e-02, -7.7425e-02,  5.3423e-02,\n",
       "                        9.7596e-02, -3.8662e-02,  1.4082e-02,  2.5632e-02,  2.6837e-02,\n",
       "                        2.6130e-02, -3.0962e-02, -4.7309e-03,  8.9997e-02,  8.5985e-02,\n",
       "                        7.2798e-02,  1.4631e-02, -5.5716e-02,  2.5509e-02,  5.6417e-03,\n",
       "                        2.3212e-02, -2.5097e-02, -1.9446e-02,  7.1779e-02, -8.5944e-02,\n",
       "                        1.0754e-02, -4.4757e-02,  1.7930e-03,  5.1675e-02, -6.7360e-02,\n",
       "                       -4.5855e-02, -4.8097e-02, -5.0272e-02,  6.7589e-02,  3.1072e-02,\n",
       "                       -1.5306e-02,  1.7165e-02, -1.1556e-02,  7.1571e-02, -7.0510e-02,\n",
       "                        4.0208e-02,  4.4778e-03,  1.2962e-02, -8.8486e-02,  5.1713e-02,\n",
       "                       -1.6703e-02,  1.8915e-02, -3.1922e-02,  5.2045e-02, -2.5861e-02,\n",
       "                        1.8761e-02, -9.4726e-02,  7.0420e-02,  9.9821e-02,  9.3978e-02,\n",
       "                       -2.8878e-02, -7.9636e-02, -2.8809e-03,  1.7892e-02,  2.1575e-02,\n",
       "                        2.6305e-02,  3.2015e-02, -8.2906e-02,  1.1841e-02,  2.0640e-02,\n",
       "                       -6.5400e-02,  2.9047e-02, -6.0382e-02,  7.1768e-02, -1.5175e-03,\n",
       "                       -7.5476e-02, -6.6003e-02, -2.8686e-02, -5.5191e-02,  5.2605e-03,\n",
       "                        6.1390e-02, -1.1857e-02,  1.5670e-03,  9.6108e-03,  8.1517e-02,\n",
       "                       -6.5608e-02,  4.6004e-02,  7.5387e-03,  5.1138e-02,  6.9214e-02,\n",
       "                        4.4218e-02,  6.2402e-02, -9.4092e-02,  2.1800e-02,  4.4615e-03,\n",
       "                        7.7855e-02,  4.0397e-02, -5.5932e-02,  6.9263e-02,  2.0134e-02,\n",
       "                       -3.4560e-02,  1.1238e-02,  6.2362e-02, -5.2047e-02,  6.0435e-02],\n",
       "                      [-5.7324e-02, -3.4384e-04,  3.0535e-02,  5.9744e-02,  4.3142e-02,\n",
       "                       -4.1975e-03,  2.9228e-02, -1.6419e-02, -4.6365e-02,  7.5520e-02,\n",
       "                        2.4160e-02,  7.6715e-02, -4.0676e-02,  4.1226e-03, -4.9552e-02,\n",
       "                       -7.9905e-02,  1.5275e-02,  4.0004e-02, -4.8387e-02, -4.5931e-02,\n",
       "                       -1.9483e-03, -7.3223e-02,  8.2302e-02, -6.9193e-03,  7.3871e-03,\n",
       "                        7.5763e-02, -4.1317e-02, -6.2873e-02,  8.6447e-02, -9.5315e-02,\n",
       "                       -9.6144e-04, -2.8185e-02, -3.7856e-02, -1.3606e-02,  8.8922e-02,\n",
       "                       -2.0505e-02,  9.0467e-02, -9.0968e-02, -7.7302e-02, -8.4227e-02,\n",
       "                        2.7471e-02, -2.4928e-02,  4.4483e-03, -7.1309e-02,  8.4659e-02,\n",
       "                        3.2405e-02,  2.6528e-02, -4.4546e-02,  8.9059e-02, -7.5788e-02,\n",
       "                       -4.1715e-02, -9.2461e-02,  6.3590e-02, -2.3931e-03,  9.3055e-02,\n",
       "                       -6.9719e-02, -1.0787e-02, -9.2717e-02,  3.3004e-02, -3.7405e-03,\n",
       "                        6.1935e-02, -8.8429e-04, -9.2992e-02,  7.2876e-02, -8.6650e-02,\n",
       "                       -6.9208e-02,  9.9348e-02,  7.4942e-02,  1.8899e-02,  3.0461e-02,\n",
       "                       -3.8166e-02, -8.4451e-02,  4.5873e-02, -9.6295e-02, -1.6700e-02,\n",
       "                       -6.8567e-02,  1.3707e-02,  4.0341e-02, -9.4632e-02,  4.1352e-02,\n",
       "                       -2.0606e-02, -3.6015e-02, -9.0255e-02, -8.5538e-02, -5.6653e-02,\n",
       "                        5.1963e-02, -2.4660e-03,  8.4608e-02, -2.1040e-02,  5.6192e-02,\n",
       "                        4.4261e-03,  9.6829e-02,  5.1640e-02, -5.2701e-02, -3.9133e-03,\n",
       "                        4.8827e-02,  5.9908e-02, -4.0238e-02, -8.3887e-02,  4.8106e-02]])),\n",
       "             ('layer3.0.bias',\n",
       "              tensor([ 0.0138,  0.0775, -0.0468, -0.0234,  0.0369,  0.0606, -0.0758, -0.0464,\n",
       "                       0.0528,  0.0326]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41f456e6-0da5-4895-b69b-4662c958b9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42658718-9329-4639-83cf-96082646fdee",
   "metadata": {},
   "source": [
    "<h2>3. Pick a loss function and optimizer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2615fb84-4220-4797-9e4e-48e45549a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5e70d7-978b-42af-b433-9cee68d5de8b",
   "metadata": {},
   "source": [
    "<h2>4. Building training loop</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3188696a-d605-4f1e-a109-abeb7a482320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Epoch 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 12\u001b[0m out \u001b[38;5;241m=\u001b[39m model(img)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out,label)\n\u001b[0;32m     14\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m---> 17\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m     18\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(a)\n\u001b[0;32m     19\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(b)\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\myApps\\anaconda3\\envs\\python3.12\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "num_epochs = 6\n",
    "for epoch in range(num_epochs):\n",
    "    print('*' * 10)\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    running_loss = 0.0\n",
    "    running_accu = 0.0\n",
    "    for i,data in enumerate(train_loader,1):\n",
    "        img,label = data\n",
    "        img = img.view(img.size(0),-1)\n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        out = model(img)\n",
    "        loss = criterion(out,label)\n",
    "        running_loss += loss.item()\n",
    "        _,pred = torch.max(out,1)\n",
    "        running_accu += (pred == label).float().mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 300 == 0:\n",
    "            print(f'Epoch {epoch + 1} / {num_epochs}, Loss: {running_loss / i:.6f}, Accu: {running_accu / i:.6f}')\n",
    "        print(f'Finish: {epoch + 1} Epoch, Loss {running_loss / i:.6f}, Accu: {running_accu / i:.6f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e250af-5bf7-454a-925f-2f884aaf1f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
