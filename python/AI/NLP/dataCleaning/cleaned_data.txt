What's going on, everybody? Welcome to part six of the Neural Networks from Scratch video series. In this video, what we're going to be talking about in covering is the Softmax activation function, which is specifically used for the output layer on our classification style neural network models. Before we get into that, a quick update to the Neural Networks from Scratch book is now fully released. We have a hardcover, which you see here, as well as a softcover. Also, the e-book now has a PDF download. All books give you access to that PDF and e-book, and we still also have the Google Docs, so you can still highlight and ask questions in line with the text and all that. If you're interested in any version of the book, you can get it at nnfs.io. I am going to be coding everything from the book, so you should be able to watch the videos, read the book, and kind of cross back and forth with very little effort. That way, if maybe you've coded something from the video, and then you're still maybe a little confused, you want to get a refresher. You could go to the end of the video. You could go in theory back to the book, for example, and kind of go through the exact same code, just in written form, or vice versa, and so on. So anyway, we're going to be starting off here with the softmax activation function. And the first question that I'm sure many people will have is, why another activation function? Why don't we just continue to use something like the rectified linear activation function? So I think to start us off, I would say, let's just... I'm going to start with this. .. a fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh, fresh. Just for illustration purposes, and then once we actually code the code that we intend to continue with our... essentially, we're building a framework here. So for that code, we'll go back to the code that we've been building. But for now, we're just going to start fresh. So let's just say you've got some layer outputs, and the values can be anything. We're just going to make them up, but I am going to use the values from the book. 4.8, 1.21, 2.385. So imagine these are your output values. What do we do with that? So if we're only predicting, it's pretty simple. You do... you just... you would say the prediction is whichever one of these values is the... is the largest. So in this case, it would be index 0. So let's assume index 0 is indeed the intended target. Awesome. You've got your prediction right. But what we're trying to learn how to do is actually train neural networks as well. So with a... you don't have to think about this if all you're doing is predicting, but someone had to think about it at some point to train this model. So if you want to train a model, the... you know, there's a whole bunch that goes on that we will get to and that we'll take up many videos and many hours of your time to figure out and learn how this works. But at this point, the first step in training a model is to determine how wrong is this model, right? Because accuracy is not a good indication of that. So for example, if we change some of these values, maybe 4.79 and 4.25. I don't know. Which one of these is more correct? Well, I think probably most people would agree. This first one is more correct because relatively this neuron zero here is relatively larger than the other neurons in this list. Whereas here, they're actually a lot closer. Accuracy wise, they're identical. So in order to measure how wrong something is, the first order of business is to compare them relatively to other neurons. But the problem with this is like the rectified linear activation function, for example, is exclusive. It's per neuron. They're not really connected in any way. So there is no relative comparison that you can really fairly make. The next problem is these are unbounded. So the relative closeness can vary considerably between each sample that you pass through this neural network. So we've got so many issues. There's like a bounding issue. Every neuron is exclusive. They don't really relate to each other. And we don't really have a good solid way of determining how wrong is this in any formal uniform way per sample that comes through. So this is the problem, and this is why we need some new activation function, which in this case we're going to be using the softmax activation function to help us solve. Now, why softmax? Well, we need to kind of, again, kind of consider what's our end objective here. So zooming out to a full model for the moment, what actually do we want to happen? So let's say we've got some image data. We're going to pass that through the neural network, and then we get the output values. Now, what do we want those values to be? So ideally, these values would be a probability distribution. This gives us a few things. First of all, this means everything will be uniform from sample to sample. Also, from neuron to neuron, things will be normalized. And we can actually calculate rightness and wrongness, so to speak. So in this case, the correct classification, if everything was perfect, the correct classification would be a 1.0, and all the other neurons would read out 0.0. So now we can actually begin to measure how right or how wrong are we. So we know this is where we want to get, and the question is, how do we actually get to that point? So one immediate idea here might be, well, all you need to really do is normalize the values and create that distribution by taking each neuron's value and divide that value by the total value of all the neurons in that layer. And then you're done, right? So let's say you just use a probability distribution, and you keep old, faithful, rectified linear activation function. The problem here is if any of those values in the output layer is a negative, the rectified linear activation function is going to clip it, and it's just going to be a 0. And then when we go to create a probability distribution from that, it will always be 0, whether it was a negative 20 or negative 9,000. And then the other problem is, what if all of the values are in the negatives? There would be impossible to learn from here, and again, we haven't quite yet reached back propagation, but learning from clipped values, quote unquote, how wrong or how right is something, is very difficult because there is no meaning once you've clipped, you've lost all meaning. Was it a negative 20 before it got clipped, or was it a negative 1 million? We don't know. So then you might suggest, let's just use a linear activation function. Let's basically do nothing. The problem is, you're still going to need to figure out what the heck do you do with these negative values. So you might throw your hands up at this point and be like, let's just use absolute values. Or let's square the output, then we'll just solve this whole negative problem altogether. The problem with this is, after you have these values, you need to be able to back propagate. You need to be able to have an optimizer intelligently optimize your variables. Well, if you have a negative 9 that got absolute value to a 9, that's a big difference than a 9, right? So the variables that you would need to tweak and the directions that you would need to tweak these variables to get those changes that you want would vary. So, we can't just lose the meaning. A negative 9 is not the same as a positive 9. So, what the heck do we do? Enter exponentiation. So the exponential function is y or your output is equal to e, or Euler's number, raised to the power of x, which is your input. Euler's number is approximately 2.71828 and so on. And exponentiation is the act of applying the exponential function to some value. So, what this does for us is it solves our negatives issue by making sure no negative, or really no value, can be negative at the output of the exponential function. And it does this while not tossing away the value or the meaning of that negativity. It's still on a scale, let's say. So, the exponentiation of 1.1 is 3 or 3 in some change. The exponentiation of negative 1.1 is 0.3329 or so. So, this is more than just using absolute values or squaring the value, for example. Now, also, in theory, you don't have to use Euler's number here, and you would still solve all your problems up to this point. But Euler's number is actually going to be coming handy later on, and we'll be sure to reference that when we get there. But this is how we're solving this problem of negativity. Alright, getting back to our code, let's code the raw Python implementation for exponentiation. So, to do this, we're just going to keep the original layer outputs. And first, we need a definition for E, the constant Euler's number, and that is 2.7182846. I hope I got that right. Also, if you are in Python, you can always import math, and then rather than hard coding Euler's number that way, you could say E equals math.E. And that would be the same thing. But if they are following along in some other language, and you don't have that kind of math library that just gives you the number, you could just hard code it in that way. So, once we have that number, we want to exponentiate all each of these values. So, doing that is really basic Python here. So, we're going to say X values, we're going to make that an empty list. And then we're just going to iterate over layer outputs. So, for output in layer outputs, we're going to X values dot append E to the power. .. Actually, it's capital E to the power of whatever that output is. And then when we're all done, we can print X values. Let's go ahead and run that. And we have our exponentiated values at this point, checking the book to make sure it is identical. And it looks like it is. Okay, great. So, that's exponentiation. Really, not much to it at that point. So, the next step, once we've exponentiated these values, is to normalize the values. So, what do we mean by normalize the values? So, in our case, it's going to be a single output neurons value, divided by the sum of all of the other output neurons in that output layer. And this gives us the probability distribution that we want. But we still want to exponentiate before this point, because again, we need to get rid of all of these negative values. But we do not want to lose the meaning of the negative value. So, we're exponentiating to convert negatives to positives without actually losing the meaning of a negative value. So, continuing along in our raw Python implementation here, let's go ahead and code in normalization. So, again, normalization occurs after we've done exponentiation, which will rid us of these pesky negative values. So, to do the normalization, again, it's a single value divided by the total of all the values. So, we've got exponential values. So, what we're going to say is norm base equals the sum of those exponential values. And then we're going to say norm values. That was, yeah. We're going to do norm values dot append. And it's just going to be that value divided by the norm base. And this will give us the normalized value. So, we can print norm values. And then also, this should add up, it should add up to one, or very, very close to one. So, print a sum of norm values. We'll run that. And what we get is here is our probability distribution. So, this first, this first kind of list here, this is our exponentiated values. Then this is our normalized exponentiated values. And then this just sums it up and shows that it does indeed add up to basically one. Those values should be pretty close to what's in the book. It actually isn't exactly, but this isn't using NumPy. So, the values should line up, I think, when we're using NumPy. We'll confirm that when we get there. So, the n and fs, first of all, we're not even initializing the n and fs package. But that is part of what the init package is doing. But here, interestingly enough, even raw Python seems to vary from machine to machine. I don't think it actually matters at this stage. So, they're close, but it's like as you get to some of these ladder decimals, that's where it's differing. So, that's the raw Python implementation. And again, if you're coming from some other language, you would need to implement this in your language. But now the next thing that we're going to do is convert this to NumPy. So, what we're going to do is take, I guess, in our code, we probably are retaining e. But we can keep math.e there, outputs there. We're going to go ahead and import NumPy as np. And rather than doing, really, all of this, we delete, and x values just becomes np. x. And what are we applying this exponential function to? Layer outputs. So, by default, typically, NumPy functions, what they're going to do is, first, they will just by default, impact every value. And if you want it to be in a little bit more specific way, you can become more specific, and we'll actually be showing that very shortly. But by default, if you just do this, it's going to apply this to each value in total. So, that's a quicker way to get our exponential values. And then for the normalization values, all we need to do at this point is, we're just going to say, norm underscore values is equal to the x values divided by the np.sum of the x values. So, let's go ahead and run that real quick, and then we see that we do indeed still get the same value or same ish values. And this time, it actually does line up with what's in the book, but if you wanted to be super particular, I would imagine, we would import nnfs and nnfs.in it. These values actually do line up with what is in the book, but let's just run that one more time. And so, if you did have any variance there, hopefully that would change it. So, okay, so that lines up exactly. And as you can see, it's a little shorter code. I think it's a little more legible. But, yeah, so that is our exponentiation and then our normalization. So, to sum up everything up to this point for our coding of the softmax activation function, we have input, right, which is actually going to be the output layer of data that we're going to input into this activation function. We exponentiate those input values, so each one uniquely gets exponentiated, then we normalize, and then that becomes our output. So, the combination of this exponentiation and normalization is what makes up the softmax activation function, the formula for which is this. And hopefully this formula is actually pretty simple to understand at this point, since you've already seen the code that goes into it, and it's not really complicated in any way. Okay, so at this point, we know everything that goes into the softmax activation function, and now it is just a function of actually applying and implementing this in such a way that makes sense for our actual neural network applications. So, the main issue that we have at this stage is we are currently working with a single kind of vector here of a layer's outputs. When really, we're not going to have a single layer of outputs, we're actually not going to have a single output from a layer. We're going to have a batch of these outputs, because we're going to have a batch of inputs, and that's going to produce a batch of outputs. So, the next thing we want to do is convert this, and really all this, to work as a batch. So to do that, first let's make this an actual batch. And again, I am going to use the same values that we have in the book. So, let me paste and paste. And the second output here is an 8.9 negative 1.81 0.2, and then down here we get a 1.41 1.0510.026. Okay. Also, we can kind of clean up here. I just realized if we still have math and math.e, we don't need that anymore, because this handles it for us. So, how do we convert to batch? So for exponentiating, it turns out we don't actually need to do anything, because these NumPy functions here work by default at the individual value level. So, if we print x values, I thought I did, I thought I already completed that, I guess I didn't. There we have our values. It's actually already done for us, and it's correct. So, the next question is, okay, how do we do this step? So, x values, we don't need to change anything there for a batch, but for some, we do. So, to illustrate sum, rather than doing the sum of the exponential values, it's kind of hard to visually add these up. We're going to do the sum of... let's do this. I'm just going to comment these out. We are going to... let's just print np.sum layer outputs. Now, remember, what do we want to do here? We actually want to iterate over here, over this 2D matrix. We want to iterate over this, and do this sum, this sum, this sum, and this sum. But by default, I already told you, it's going to do as individual values. So by default, what we get is a single scalar value. That's not what we wanted. We really want three values. For sure, we want three values. So, how do we get those three values? So, the first order of business is to pass the axes parameter here. So, we're going to say axes. And the axes, by default, is actually none. And that gives us the same value that we saw before. Ax is zero. To put it extremely simply on a 2D matrix, it's going to be the sum of columns. So, we're going to run this again. And as you can see, 15.11 is this column, and then 0.451 is this column, and 2. 611 is this last column. Unfortunately, that's not what we want. We actually want the sum of the rows put simply. So, we can change that to axes one. And sure enough, what we get is what we hoped for, which is the sum here, here, and here. Now, we still have a slight problem. What are we trying to do with this sum? Well, it turns out we're trying to take these exponential values, which has this shape. It is a matrix. And we're attempting to divide it by what we do the sum operation with. And we don't actually want to just willy-nilly divide it. It's really important that things line up. So, it's for the same reason that we want to make sure values are lining up when we do the dot product, and when we're doing like a matrix product, right? So, even though NP dot dot, it is a dot product with vectors, but it's also doing a matrix product for us, we need those values to line up. So, we do a transpose, right? For that same reason, we need the right values to line up here. Right now, if we did this, it's not going to actually be dividing what we want exponentials to be divided by, right? So, what we want to do is we need to shape this correctly. And we could reshape this, but we can also just simply use keep-dems. So, keep-is it keep-dems? Yeah, all one word. And then true, run that. And now, it is a matrix of the exact same- I hate to say shape, but it's the same orientation. It's the same dimensions, okay? So, it's just a sum. So now, it is literally this right here. It's just the sum of these values, the sum of these values, and then the sum of these values. So, with that, what we can do is we can say the norm values is exponential values divided by the sum of exponential values at axes one, and then keep-dems equals true. We get rid of this row here, and now we have print norm values. We run that, and now we have our actual normalized values. So, from this point, we really have one more thing, and I think we'll cover it here, is one problem that we have up to this point is with specifically exponential values. So, there's one thing that we want to change with exponential values before we convert this to our actual class object. So, with that, let us go ahead and talk about what we're going to do with exponential values. One slight issue with exponentiation is the explosion of values as the input to the exponential function grows. It doesn't take much to get massive numbers, and even worse, it doesn't take too long to reach an overflow. So, one way to combat this overflow is to take all of the values in this output layer prior to exponentiation, and subtract the largest value in that layer from all of the values in that layer. And what this causes is now the largest value will be a zero, and everything else is going to be less than zero. Now, because the largest value prior to exponentiation is actually a zero, our range of possibilities becomes somewhere between zero and one after exponentiation, because the exponentiation of zero equals one. So, this means our range of values can only be between zero and one, thus no more worrying about overflowing. So, the final concern that you might have is what sort of impact does subtracting the max value from everything have on the actual output of the softmax activation function. So, all other things being equal and assuming that we don't have some sort of overflow error, if we have two output layers and one we don't subtract the max from and one we do, after we do our exponentiation and our normalization, the actual output is identically the same. The only thing we've done is protected ourselves from an actual overflow error. Alright, so with the subtraction of the max value in mind, we are going to now go to the code that we left off on with part five and we're going to add our softmax activation class. And then we're going to implement it here as well as change things here. I've used different variable names and stuff like that. So, I'm going to attempt to convert this to be exactly what we have in the book as well. And so, by doing that we'll also be able to finally test the softmax activation function. So, these are the changes that we want to make. So, first off, we're going to be redefining this data. So, I'm actually going to delete these. Then, we'll just add this underneath the value activation function. And we're going to say class activation, activation underscore softmax. And then we're going to define the forward method, again, self-inputs. And now we're just going to implement the code that we've talked about and covered already up to this point, except for the max thing, but I'm going to show you that in a moment. So, the exponential underscore values is going to be equal to np dot x of inputs. And then we want to minus the np dot max. And immediately alarm bells should be sort of going off. And that is, if we run np dot max, and we, for example, we're going to say np dot max inputs. And also, while we're here, what is inputs? So, up to this point, inputs, in this case, because we're using the softmax activation function for the activation function of an output layer, inputs are actually the outputs so far of our model, right? And so, again, we've got these outputs, and those outputs are going to be in batch form. So, there will be a batch of these outputs. So, if we just said np dot max of inputs, which would be np dot max of a batch of outputs, up to this point, it's going to be the highest value of all of those values. Now, you'd probably get away with it. I think it would probably work, but it would be a mistake. It would work, but not as well as it should. So, instead of what you want to say is np dot max inputs of, and immediately you should already know where we're headed, it's going to be of axes one, and then, again, keep dems will be true. So, we're going to take inputs, which is going to be this batch of inputs. And, again, we want to subtract the max along of axes one, and then we want to keep that dimension, right, of that array. So, once we've done that, we now have our exponential values of a batch, and we are subtracting the max along the way. Again, so we just don't hit an overflow value. The actual outputs will be exactly the same after we normalize all that. So, we're not really losing anything by doing this. We're just preventing an overflow. So, exponential values is done. Now we're going to say probability is going to be equal to x values divided by np dot sum of the exponential values of axes one, axes one, keep dems is going to be true. And, again, we've already actually... this we've already covered, but again, we're trying to sum each of the rows, and we want to keep the dimension. So, finally, we're going to define our self dot output, and that is going to be the probabilities. Okay. So, that's our soft max activation function, and now what I want to make sure I do is I use the exact same layer values and so on so. We already got rid of the code up above, I think. I think what I'll do... I think we'll just code it from scratch here. So, here is our framework up to this point. Well, first thing we're going to do is define our data. So, x, y is going to be equal to spiral data, and then we're going to say samples. How many samples do we want to have? We're going to say 100 per class, and we're going to say classes. We're going to have three separate classes. Now, what we're going to do is we're going to say dense one is equal to layer, dense, and the shape here is going to be two, and then three. So, we only have two input features. Again, spiral data is just x, y data, so it's just coordinates. So, the input here is going to be x, y, that's two, so the input must be two. Now, the output could be anything you want. You could say 90 if you wanted, but we're going to say three because we're going to follow, we're going to say three. Because we're going to follow the book. Now, after we've got dense one, we're going to define also activation one, and that will be an activation value. And then we're going to define dense two is going to be another layer dense. It, the input, because the output of the previous layer was a three, the input must be three. Now, the output could be anything we want, but we're going to treat this like it's an output layer. So, how many neurons should this output layer have? Well, we have three classes, so we're going to say three. So, dense two is defined, and then we're going to say activation, activation two is going to be an activation softmax. And then now, we're going to begin passing data actually through here. So, we're going to say dense one, dense one dot forward, and we're going to forward our actual input data here. Then, we need to activate it. So, activation one dot forward, dense one dot output. Then, we're going to pass that through dense two. So, we're going to say dense two dot forward, activation one dot output. And then, activation two dot forward the output from dense two. And, at this point, because we do the forward, we now have the output, which is going to be probabilities. So, for example, we can print activation two dot output, and let's just do the, there should be three hundred of them. So, we're going to just do the first five. Let's go ahead and run that. And, what we get is, again, this is a batch. Okay, so we passed all of them at the same time, in this case. So, we, theory, had a batch of three hundred. Now, each of these is unique. So, this is a sample. So, this had an input of two values, and this is each neuron value of those two values. And, it just continues to repeat, and then this is just the first five. Now, it should be no surprise. Everything was initialized randomly. So, it turns out that the distribution of predictions is indeed random. It's a perfect, you know, not a perfect, but very close to a perfect one-third prediction for everything. This is totally normal. When you randomly initialize a model, that's kind of what you expect. So, now what we need to do is actually train this model. Because, like, for example, let's see if any of these are obviously bigger than the others. Three, three, three, five. So, in this case, right, if we did an argmax on this right here, this class, you know, of index two, is the largest, unless I'm missing something, but anyways, it is the largest it looks like. So, that would be the prediction, and maybe that would be accurate. Maybe that would be correct. But instead, what we want to really do is we don't want to know just what's right and wrong. We want to know how right and how wrong, and to calculate that, we use a loss function. And that is the topic of the next video. So, if you've got any questions, comments, concerns, whatever, up to this point, we kind of covered a lot. But I think the Softmax activation function is relatively simplistic to pick up. But if you have questions, comments, concerns, whatever, feel free to leave those below. Again, the physical versions of the books are done. As are obviously the e-book and all that. It's ready to be downloaded and consumed. So, if you want something to follow along with the videos, or you want to kind of refresh from the videos, or read before the video, watch the video, that kind of stuff, definitely check that out at nnfs .io. Otherwise, I will see you guys in another video.  